{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Combine an *n-gram* with Wav2Vec2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we want to wrap the *5-gram* into a `Wav2Vec2ProcessorWithLM` object to make the *5-gram* boosted decoding as seamless as shown in Section 1.\n",
    "We start by downloading the currently \"LM-less\" processor of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyctcdecode\n",
      "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyctcdecode) (1.26.2)\n",
      "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode)\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Collecting hypothesis<7,>=6.14 (from pyctcdecode)\n",
      "  Downloading hypothesis-6.97.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (23.1.0)\n",
      "Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis<7,>=6.14->pyctcdecode)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.2.0)\n",
      "Downloading hypothesis-6.97.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.3/436.3 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, pygtrie, hypothesis, pyctcdecode\n",
      "Successfully installed hypothesis-6.97.1 pyctcdecode-0.5.0 pygtrie-2.5.0 sortedcontainers-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kenlm\n",
      "  Downloading kenlm-0.2.0.tar.gz (427 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.4/427.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=597536 sha256=03ba5e26a8afaa4150a6c9134736f4980450cca2742d8dc5f19f56c7688e0b42\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/fd/80/e0/18f4148e863fb137bd87e21ee2bf423b81b3ed6989dab95135\n",
      "Successfully built kenlm\n",
      "Installing collected packages: kenlm\n",
      "Successfully installed kenlm-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kenlm -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the vocabulary of its tokenizer as it represents the `\"labels\"` of `pyctcdecode`'s `BeamSearchDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"labels\"` and the previously built `5gram_correct.arpa` file is all that's needed to build the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/FinalProject/word2vec-kenlm/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely ignore the warning and all that is left to do now is to wrap the just created `decoder`, together with the processor's `tokenizer` and `feature_extractor` into a `Wav2Vec2ProcessorWithLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to directly upload the LM-boosted processor into\n",
    "the model folder of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv) to have all relevant files in one place.\n",
    "\n",
    "Let's clone the repo, add the new decoder files and upload them afterward.\n",
    "First, we need to install `git-lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install git-lfs tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning and uploading of modeling files can be done conveniently with the `huggingface_hub`'s `Repository` class.\n",
    "\n",
    "More information on how to use the `huggingface_hub` to upload any files, please take a look at the [official docs](https://huggingface.co/docs/hub/how-to-upstream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/hf-test/xls-r-300m-sv into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import Repository\n",
    "\n",
    "# repo = Repository(local_dir=\"xls-r-300m-sv\", clone_from=\"hf-test/xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cloned `xls-r-300m-sv`, let's save the new processor with LM into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor_with_lm.save_pretrained(\"xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the local repository. The `tree` command conveniently can also show the size of the different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xls-r-300m-sv/\n",
      "├── [  23]  added_tokens.json\n",
      "├── [ 401]  all_results.json\n",
      "├── [ 253]  alphabet.json\n",
      "├── [2.0K]  config.json\n",
      "├── [ 304]  emissions.csv\n",
      "├── [ 226]  eval_results.json\n",
      "├── [4.0K]  language_model\n",
      "│   ├── [4.1G]  5gram_correct.arpa\n",
      "│   ├── [  78]  attrs.json\n",
      "│   └── [4.9M]  unigrams.txt\n",
      "├── [ 240]  preprocessor_config.json\n",
      "├── [1.2G]  pytorch_model.bin\n",
      "├── [3.5K]  README.md\n",
      "├── [4.0K]  runs\n",
      "│   └── [4.0K]  Jan09_22-00-50_brutasse\n",
      "│       ├── [4.0K]  1641765760.8871996\n",
      "│       │   └── [4.6K]  events.out.tfevents.1641765760.brutasse.31164.1\n",
      "│       ├── [ 42K]  events.out.tfevents.1641765760.brutasse.31164.0\n",
      "│       └── [ 364]  events.out.tfevents.1641794162.brutasse.31164.2\n",
      "├── [1.2K]  run.sh\n",
      "├── [ 30K]  run_speech_recognition_ctc.py\n",
      "├── [ 502]  special_tokens_map.json\n",
      "├── [ 279]  tokenizer_config.json\n",
      "├── [ 29K]  trainer_state.json\n",
      "├── [2.9K]  training_args.bin\n",
      "├── [ 196]  train_results.json\n",
      "├── [ 319]  vocab.json\n",
      "└── [4.0K]  wandb\n",
      "    ├── [  52]  debug-internal.log -> run-20220109_220240-1g372i3v/logs/debug-internal.log\n",
      "    ├── [  43]  debug.log -> run-20220109_220240-1g372i3v/logs/debug.log\n",
      "    ├── [  28]  latest-run -> run-20220109_220240-1g372i3v\n",
      "    └── [4.0K]  run-20220109_220240-1g372i3v\n",
      "        ├── [4.0K]  files\n",
      "        │   ├── [8.8K]  conda-environment.yaml\n",
      "        │   ├── [140K]  config.yaml\n",
      "        │   ├── [4.7M]  output.log\n",
      "        │   ├── [5.4K]  requirements.txt\n",
      "        │   ├── [2.1K]  wandb-metadata.json\n",
      "        │   └── [653K]  wandb-summary.json\n",
      "        ├── [4.0K]  logs\n",
      "        │   ├── [3.4M]  debug-internal.log\n",
      "        │   └── [8.2K]  debug.log\n",
      "        └── [113M]  run-1g372i3v.wandb\n",
      "\n",
      "9 directories, 34 files\n"
     ]
    }
   ],
   "source": [
    "# !tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the *5-gram* LM is quite large - it amounts to more than 4 GB.\n",
    "To reduce the size of the *n-gram* and make loading faster, `kenLM` allows converting `.arpa` files to binary ones using the `build_binary` executable.\n",
    "\n",
    "Let's make use of it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading xls-r-300m-sv/language_model/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# !kenlm/build/bin/build_binary xls-r-300m-sv/language_model/5gram_correct.arpa xls-r-300m-sv/language_model/5gram.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it worked! Let's remove the `.arpa` file and check the size of the binary *5-gram* LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xls-r-300m-sv/\n",
      "├── [  23]  added_tokens.json\n",
      "├── [ 401]  all_results.json\n",
      "├── [ 253]  alphabet.json\n",
      "├── [2.0K]  config.json\n",
      "├── [ 304]  emissions.csv\n",
      "├── [ 226]  eval_results.json\n",
      "├── [4.0K]  language_model\n",
      "│   ├── [1.8G]  5gram.bin\n",
      "│   ├── [  78]  attrs.json\n",
      "│   └── [4.9M]  unigrams.txt\n",
      "├── [ 240]  preprocessor_config.json\n",
      "├── [1.2G]  pytorch_model.bin\n",
      "├── [3.5K]  README.md\n",
      "├── [4.0K]  runs\n",
      "│   └── [4.0K]  Jan09_22-00-50_brutasse\n",
      "│       ├── [4.0K]  1641765760.8871996\n",
      "│       │   └── [4.6K]  events.out.tfevents.1641765760.brutasse.31164.1\n",
      "│       ├── [ 42K]  events.out.tfevents.1641765760.brutasse.31164.0\n",
      "│       └── [ 364]  events.out.tfevents.1641794162.brutasse.31164.2\n",
      "├── [1.2K]  run.sh\n",
      "├── [ 30K]  run_speech_recognition_ctc.py\n",
      "├── [ 502]  special_tokens_map.json\n",
      "├── [ 279]  tokenizer_config.json\n",
      "├── [ 29K]  trainer_state.json\n",
      "├── [2.9K]  training_args.bin\n",
      "├── [ 196]  train_results.json\n",
      "├── [ 319]  vocab.json\n",
      "└── [4.0K]  wandb\n",
      "    ├── [  52]  debug-internal.log -> run-20220109_220240-1g372i3v/logs/debug-internal.log\n",
      "    ├── [  43]  debug.log -> run-20220109_220240-1g372i3v/logs/debug.log\n",
      "    ├── [  28]  latest-run -> run-20220109_220240-1g372i3v\n",
      "    └── [4.0K]  run-20220109_220240-1g372i3v\n",
      "        ├── [4.0K]  files\n",
      "        │   ├── [8.8K]  conda-environment.yaml\n",
      "        │   ├── [140K]  config.yaml\n",
      "        │   ├── [4.7M]  output.log\n",
      "        │   ├── [5.4K]  requirements.txt\n",
      "        │   ├── [2.1K]  wandb-metadata.json\n",
      "        │   └── [653K]  wandb-summary.json\n",
      "        ├── [4.0K]  logs\n",
      "        │   ├── [3.4M]  debug-internal.log\n",
      "        │   └── [8.2K]  debug.log\n",
      "        └── [113M]  run-1g372i3v.wandb\n",
      "\n",
      "9 directories, 34 files\n"
     ]
    }
   ],
   "source": [
    "# !rm xls-r-300m-sv/language_model/5gram_correct.arpa && tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we reduced the *n-gram* by more than half to less than 2GB now. In the final step, let's upload all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS: (1 of 1 files) 1.85 GB / 1.85 GB\n",
      "Counting objects: 9, done.\n",
      "Delta compression using up to 2 threads.\n",
      "Compressing objects: 100% (9/9), done.\n",
      "Writing objects: 100% (9/9), 1.23 MiB | 1.92 MiB/s, done.\n",
      "Total 9 (delta 3), reused 0 (delta 0)\n",
      "To https://huggingface.co/hf-test/xls-r-300m-sv\n",
      "   27d0c57..5a191e2  main -> main\n"
     ]
    }
   ],
   "source": [
    "# repo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Now you should be able to use the *5gram* for LM-boosted decoding as shown in Section 1.\n",
    "\n",
    "As can be seen on [`xls-r-300m-sv`'s model card](https://huggingface.co/hf-test/xls-r-300m-sv#inference-with-lm) our *5gram* LM-boosted decoder yields a WER of 18.85% on Common Voice's 7 test set which is a relative performance of *ca.* 30% 🔥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'audio_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2ForCTC\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2ForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimvladikon/wav2vec2-xls-r-300m-hebrew\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m transcription \u001b[38;5;241m=\u001b[39m processor_with_lm(\u001b[43maudio_sample\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m], sampling_rate\u001b[38;5;241m=\u001b[39maudio_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_values\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(audio_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m], sampling_rate\u001b[38;5;241m=\u001b[39maudio_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# #run inference on test dataset first example\n",
    "# import soundfile as sf\n",
    "# import torch\n",
    "# from IPython.display import Audio\n",
    "\n",
    "# test_dataset = datasets.load_from_disk(\"/teamspace/studios/this_studio/FinalProject/datasets/kan_dataset/test\")\n",
    "\n",
    "# audio_sample = test_dataset[2]\n",
    "# audio_sentence = audio_sample[\"sentence\"]\n",
    "# # print(audio_sample[\"text\"].lower())\n",
    "# print(audio_sentence)\n",
    "# Audio(data=audio_sample[\"audio\"][\"array\"], autoplay=True, rate=audio_sample[\"audio\"][\"sampling_rate\"])\n",
    "# from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")\n",
    "# transcription = processor_with_lm(audio_sample[\"audio\"][\"array\"], sampling_rate=audio_sample[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\").input_values\n",
    "\n",
    "# inputs = processor(audio_sample[\"audio\"][\"array\"], sampling_rate=audio_sample[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "\n",
    "# import torch\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   logits = model(**inputs).logits\n",
    "# # predicted_ids = torch.argmax(logits, dim=-1)\n",
    "# transcription = processor_with_lm.batch_decode(logits.numpy()).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'היום נחלק את השיעור לשלושה חלקים בשני החלקים הראשונים נדבר על השלב שקובע למעשר בגידולים שונים בחלק השלישי נראה מאמר מוסגר לגבי היכולת לדייק בשיעורי דרבנן \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"dafyomi_test.txt\") as f:\n",
    "#     lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
