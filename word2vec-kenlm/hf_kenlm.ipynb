{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Getting data for your language model**\n",
    "\n",
    "A language model that is useful for a speech recognition system should support the acoustic model, *e.g.* Wav2Vec2, in predicting the next word (or token, letter) and therefore model the following distribution:\n",
    "\n",
    "$\\mathbf{P}(w_n | \\mathbf{w}_0^{t-1})$ with $w_n$ being the next word and $\\mathbf{w}_0^{t-1}$ being the sequence of all previous words since the beginning of the utterance. Simply said, the language model should be good at predicting the next word given all previously transcribed words regardless of the audio input given to the speech recognition system.\n",
    "\n",
    "As always a language model is only as good as the data it is trained on. In the case of speech recognition, we should therefore ask ourselves for what kind of data, the speech recognition will be used for: *conversations*, *audiobooks*, *movies*, *speeches*, *, etc*, ...?\n",
    "\n",
    "The language model should be good at modeling language that corresponds to the\n",
    "target transcriptions of the speech recognition system.\n",
    "For demonstration purposes, we assume here that we have fine-tuned a pre-trained [`facebook/wav2vec2-xls-r-300m`](https://huggingface.co/facebook/wav2vec2-xls-r-300m) on [Common Voice 7](https://huggingface.co/datasets/mozilla-foundation/common_voice_7_0) in Swedish. The fine-tuned checkpoint can\n",
    "be found [here](https://huggingface.co/hf-test/xls-r-300m-sv).\n",
    "Common Voice 7 is a relatively crowd-sourced read-out audio dataset and we will evaluate the model on its test data.\n",
    "\n",
    "Let's now look for suitable text data on the Hugging Face Hub. We search all datasets for those [that contain Swedish data](https://huggingface.co/datasets?languages=languages:sv&sort=downloads).\n",
    "Browsing a bit through the datasets, we are looking for a dataset that is similar to Common Voice's read-out audio data. The obvious choices of [oscar](https://huggingface.co/datasets/oscar) and [mc4](https://huggingface.co/datasets/mc4) might not be the most suitable here because they:\n",
    "\n",
    "- are generated from crawling the web, which might not be very clean and correspond well to spoken language\n",
    "- require a lot of pre-processing\n",
    "- are very large which is not ideal for demonstration purposes here 😉\n",
    "\n",
    "A dataset that seems sensible here and which is relatively clean and easy to pre-process is [europarl_bilingual](https://huggingface.co/datasets/europarl_bilingual) as it's a dataset that is based on discussions and talks of the European parliament. It should therefore be relatively clean and correspond well to read-out audio data. The dataset is originally designed for machine translation and can therefore only be accessed in translation pairs. We will only extract the text of the target language, Swedish (`sv`), from the *English-to-Swedish* translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lang=\"sv\"  # change to your target lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9a1f9160fc4dedb2ef5e9b9d6770d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc34c4c34d6242959ac953b84b7c3404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/127M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfb172816b74e5ead31473994aa52d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c00d0cc165e42f289e69211da69aa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1892723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 1892723\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"europarl_bilingual\", lang1=\"en\", lang2=target_lang, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is quite large - it has over a million translations. Since it's only text data, it should be relatively easy to process though.\n",
    "\n",
    "Next, let's look at how the data was preprocessed when training the fine-tuned *XLS-R* checkpoint in Swedish. Looking at the [`run.sh` file](https://huggingface.co/hf-test/xls-r-300m-sv/blob/main/run.sh), we can see that the following characters were removed from the official transcriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[,?.!\\-\\;\\:\\\"“%‘”�—’…–]'  # change to the ignored characters of your fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same here so that the alphabet of our language model matches the one of the fine-tuned acoustic checkpoints.\n",
    "\n",
    "We can write a single map function to extract the Swedish text and process it right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_text(batch):\n",
    "  text = batch[\"translation\"][target_lang]\n",
    "  batch[\"text\"] = re.sub(chars_to_ignore_regex, \"\", text.lower())\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the `.map()` function. This should take roughly 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e115e1d9b2864d2c87091fba897965d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1892723 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(extract_text, remove_columns=dataset.column_names)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Our dataset is already finished. Let's upload it to the Hub so that we can inspect and reuse it better.\n",
    "\n",
    "You can log in by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /root/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call 🤗 Hugging Face's [`push_to_hub`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=push#datasets.Dataset.push_to_hub) method to upload the dataset to the repo `\"swedish_corpora_parliament_processed\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bffcb75f5504c11982df20cc1436d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.push_to_hub(f\"{target_lang}_corpora_parliament_processed\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! The dataset viewer is automatically enabled when uploading a new dataset, which is very convenient. You can now directly inspect the dataset online.\n",
    "\n",
    "Feel free to look through our preprocessed dataset directly on [`hf-test/sv_corpora_parliament_processed`](https://huggingface.co/datasets/hf-test/sv_corpora_parliament_processed). Even if we are not a native speaker in Swedish, we can see that the data is well processed and seems clean.\n",
    "\n",
    "Next, let's use the data to build a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Build an *n-gram* with KenLM**\n",
    "\n",
    "While large language models based on the [Transformer architecture](https://jalammar.github.io/illustrated-transformer/) have become the standard in NLP, it is still very common to use an ***n-gram*** LM to boost speech recognition systems - as shown in Section 1.\n",
    "\n",
    "Looking again at Table 9 of Appendix C of the [official Wav2Vec2 paper](https://arxiv.org/abs/2006.11477), it can be noticed that using a *Transformer*-based LM for decoding clearly yields better results than using an *n-gram* model, but the difference between *n-gram* and *Transformer*-based LM is much less significant than the difference between *n-gram* and no LM.\n",
    "\n",
    "*E.g.*, for the large Wav2Vec2 checkpoint that was fine-tuned on 10min only, an *n-gram* reduces the word error rate (WER) compared to no LM by *ca.* 80% while a *Transformer*-based LM *only* reduces the WER by another 23% compared to the *n-gram*. This relative WER reduction becomes less, the more data the acoustic model has been trained on. *E.g.*, for the large checkpoint a *Transformer*-based LM reduces the WER by merely 8% compared to an *n-gram* LM whereas the *n-gram* still yields a 21% WER reduction compared to no language model.\n",
    "\n",
    "The reason why an *n-gram* is preferred over a *Transformer*-based LM is that *n-grams* come at a significantly smaller computational cost. For an *n-gram*, retrieving the probability of a word given previous words is almost only as computationally expensive as querying a look-up table or tree-like data storage - *i.e.* it's very fast compared to modern *Transformer*-based language models that would require a full forward pass to retrieve the next word probabilities.\n",
    "\n",
    "For more information on how *n-grams* function and why they are (still) so useful for speech recognition, the reader is advised to take a look at [this excellent summary](https://web.stanford.edu/~jurafsky/slp3/3.pdf) from Stanford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's see step-by-step how to build an *n-gram*. We will use the popular [KenLM library](https://github.com/kpu/kenlm) to do so. Let's start by installing the Ubuntu library prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before downloading and unpacking the KenLM repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KenLM is written in C++, so we'll make use of `cmake` to build the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
    "!ls kenlm/build/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, as we can see, the executable functions have successfully been built under `kenlm/build/bin/`.\n",
    "\n",
    "KenLM by default computes an *n-gram* with [Kneser-Ney smooting](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing). All text data used to create the *n-gram* is expected to be stored in a text file.\n",
    "We download our dataset and save it as a `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "username = \"hf-test\"  # change to your username\n",
    "\n",
    "dataset = load_dataset(f\"{username}/{target_lang}_corpora_parliament_processed\", split=\"train\")\n",
    "\n",
    "with open(\"text.txt\", \"w\") as file:\n",
    "  file.write(\" \".join(dataset[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just have to run KenLM's `lmplz` command to build our *n-gram*, called `\"5gram.arpa\"`. As it's relatively common in speech recognition, we build a *5-gram* by passing the `-o 5` parameter.\n",
    "For more information on the different *n-gram* LM that can be built\n",
    "with KenLM, one can take a look at the [official website of KenLM](https://kheafield.com/code/kenlm/).\n",
    "\n",
    "Executing the command below might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /content/swedish_text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "tcmalloc: large alloc 1918697472 bytes == 0x55d40d0f0000 @  0x7fdccb1a91e7 0x55d40b2f17a2 0x55d40b28c51e 0x55d40b26b2eb 0x55d40b257066 0x7fdcc9342bf7 0x55d40b258baa\n",
      "tcmalloc: large alloc 8953896960 bytes == 0x55d47f6c0000 @  0x7fdccb1a91e7 0x55d40b2f17a2 0x55d40b2e07ca 0x55d40b2e1208 0x55d40b26b308 0x55d40b257066 0x7fdcc9342bf7 0x55d40b258baa\n",
      "****************************************************************************************************\n",
      "Unigram tokens 42153890 types 360209\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:4322508 2:1062772928 3:1992699264 4:3188318720 5:4649631744\n",
      "tcmalloc: large alloc 4649631744 bytes == 0x55d40d0f0000 @  0x7fdccb1a91e7 0x55d40b2f17a2 0x55d40b2e07ca 0x55d40b2e1208 0x55d40b26b8d7 0x55d40b257066 0x7fdcc9342bf7 0x55d40b258baa\n",
      "tcmalloc: large alloc 1992704000 bytes == 0x55d561ce0000 @  0x7fdccb1a91e7 0x55d40b2f17a2 0x55d40b2e07ca 0x55d40b2e1208 0x55d40b26bcdd 0x55d40b257066 0x7fdcc9342bf7 0x55d40b258baa\n",
      "tcmalloc: large alloc 3188326400 bytes == 0x55d695a86000 @  0x7fdccb1a91e7 0x55d40b2f17a2 0x55d40b2e07ca 0x55d40b2e1208 0x55d40b26bcdd 0x55d40b257066 0x7fdcc9342bf7 0x55d40b258baa\n",
      "Statistics:\n",
      "1 360208 D1=0.686222 D2=1.01595 D3+=1.33685\n",
      "2 5476741 D1=0.761523 D2=1.06735 D3+=1.32559\n",
      "3 18177681 D1=0.839918 D2=1.12061 D3+=1.33794\n",
      "4 30374983 D1=0.909146 D2=1.20496 D3+=1.37235\n",
      "5 37231651 D1=0.944104 D2=1.25164 D3+=1.344\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 1884 assuming -p 1.5\n",
      "probing 2195 assuming -r models -p 1.5\n",
      "trie     922 without quantization\n",
      "trie     518 assuming -q 8 -b 8 quantization \n",
      "trie     806 assuming -a 22 array pointer compression\n",
      "trie     401 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:4322496 2:87627856 3:363553620 4:728999592 5:1042486228\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:4322496 2:87627856 3:363553620 4:728999592 5:1042486228\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:14181536 kB\tVmRSS:2199260 kB\tRSSMax:4160328 kB\tuser:120.598\tsys:26.6659\tCPU:147.264\treal:136.344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!kenlm/build/bin/lmplz -o 5 <\"text.txt\" > \"5gram.arpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have built a *5-gram* LM! Let's inspect the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=360208\n",
      "ngram 2=5476741\n",
      "ngram 3=18177681\n",
      "ngram 4=30374983\n",
      "ngram 5=37231651\n",
      "\n",
      "\\1-grams:\n",
      "-6.770219\t<unk>\t0\n",
      "0\t<s>\t-0.11831701\n",
      "-4.6095004\tåterupptagande\t-1.2174699\n",
      "-2.2361007\tav\t-0.79668784\n",
      "-4.8163533\tsessionen\t-0.37327805\n",
      "-2.2251768\tjag\t-1.4205662\n",
      "-4.181505\tförklarar\t-0.56261665\n",
      "-3.5790775\teuropaparlamentets\t-0.63611007\n",
      "-4.771945\tsession\t-0.3647111\n",
      "-5.8043895\tåterupptagen\t-0.3058712\n",
      "-2.8580177\tefter\t-0.7557702\n",
      "-5.199537\tavbrottet\t-0.43322718\n"
     ]
    }
   ],
   "source": [
    "!head -20 5gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small problem that 🤗 Transformers will not be happy about later on.\n",
    "The *5-gram* correctly includes a \"Unknown\" or `<unk>`, as well as a *begin-of-sentence*, `<s>` token, but no *end-of-sentence*, `</s>` token.\n",
    "This sadly has to be corrected currently after the build.\n",
    "\n",
    "We can simply add the *end-of-sentence* token by adding the line `0 </s>  -0.11831701` below the *begin-of-sentence* token and increasing the `ngram 1` count by 1. Because the file has roughly 100 million lines, this command will take *ca.* 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the corrected *5-gram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=360209\n",
      "ngram 2=5476741\n",
      "ngram 3=18177681\n",
      "ngram 4=30374983\n",
      "ngram 5=37231651\n",
      "\n",
      "\\1-grams:\n",
      "-6.770219\t<unk>\t0\n",
      "0\t<s>\t-0.11831701\n",
      "0\t</s>\t-0.11831701\n",
      "-4.6095004\tåterupptagande\t-1.2174699\n",
      "-2.2361007\tav\t-0.79668784\n",
      "-4.8163533\tsessionen\t-0.37327805\n",
      "-2.2251768\tjag\t-1.4205662\n",
      "-4.181505\tförklarar\t-0.56261665\n",
      "-3.5790775\teuropaparlamentets\t-0.63611007\n",
      "-4.771945\tsession\t-0.3647111\n",
      "-5.8043895\tåterupptagen\t-0.3058712\n",
      "-2.8580177\tefter\t-0.7557702\n"
     ]
    }
   ],
   "source": [
    "!head -20 5gram_correct.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this looks better! We're done at this point and all that is left to do is to correctly integrate the `\"ngram\"` with [`pyctcdecode`](https://github.com/kensho-technologies/pyctcdecode) and 🤗 Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Combine an *n-gram* with Wav2Vec2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we want to wrap the *5-gram* into a `Wav2Vec2ProcessorWithLM` object to make the *5-gram* boosted decoding as seamless as shown in Section 1.\n",
    "We start by downloading the currently \"LM-less\" processor of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"hf-test/xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the vocabulary of its tokenizer as it represents the `\"labels\"` of `pyctcdecode`'s `BeamSearchDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"labels\"` and the previously built `5gram_correct.arpa` file is all that's needed to build the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely ignore the warning and all that is left to do now is to wrap the just created `decoder`, together with the processor's `tokenizer` and `feature_extractor` into a `Wav2Vec2ProcessorWithLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to directly upload the LM-boosted processor into\n",
    "the model folder of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv) to have all relevant files in one place.\n",
    "\n",
    "Let's clone the repo, add the new decoder files and upload them afterward.\n",
    "First, we need to install `git-lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install git-lfs tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning and uploading of modeling files can be done conveniently with the `huggingface_hub`'s `Repository` class.\n",
    "\n",
    "More information on how to use the `huggingface_hub` to upload any files, please take a look at the [official docs](https://huggingface.co/docs/hub/how-to-upstream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/hf-test/xls-r-300m-sv into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(local_dir=\"xls-r-300m-sv\", clone_from=\"hf-test/xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cloned `xls-r-300m-sv`, let's save the new processor with LM into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_with_lm.save_pretrained(\"xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the local repository. The `tree` command conveniently can also show the size of the different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xls-r-300m-sv/\n",
      "├── [  23]  added_tokens.json\n",
      "├── [ 401]  all_results.json\n",
      "├── [ 253]  alphabet.json\n",
      "├── [2.0K]  config.json\n",
      "├── [ 304]  emissions.csv\n",
      "├── [ 226]  eval_results.json\n",
      "├── [4.0K]  language_model\n",
      "│   ├── [4.1G]  5gram_correct.arpa\n",
      "│   ├── [  78]  attrs.json\n",
      "│   └── [4.9M]  unigrams.txt\n",
      "├── [ 240]  preprocessor_config.json\n",
      "├── [1.2G]  pytorch_model.bin\n",
      "├── [3.5K]  README.md\n",
      "├── [4.0K]  runs\n",
      "│   └── [4.0K]  Jan09_22-00-50_brutasse\n",
      "│       ├── [4.0K]  1641765760.8871996\n",
      "│       │   └── [4.6K]  events.out.tfevents.1641765760.brutasse.31164.1\n",
      "│       ├── [ 42K]  events.out.tfevents.1641765760.brutasse.31164.0\n",
      "│       └── [ 364]  events.out.tfevents.1641794162.brutasse.31164.2\n",
      "├── [1.2K]  run.sh\n",
      "├── [ 30K]  run_speech_recognition_ctc.py\n",
      "├── [ 502]  special_tokens_map.json\n",
      "├── [ 279]  tokenizer_config.json\n",
      "├── [ 29K]  trainer_state.json\n",
      "├── [2.9K]  training_args.bin\n",
      "├── [ 196]  train_results.json\n",
      "├── [ 319]  vocab.json\n",
      "└── [4.0K]  wandb\n",
      "    ├── [  52]  debug-internal.log -> run-20220109_220240-1g372i3v/logs/debug-internal.log\n",
      "    ├── [  43]  debug.log -> run-20220109_220240-1g372i3v/logs/debug.log\n",
      "    ├── [  28]  latest-run -> run-20220109_220240-1g372i3v\n",
      "    └── [4.0K]  run-20220109_220240-1g372i3v\n",
      "        ├── [4.0K]  files\n",
      "        │   ├── [8.8K]  conda-environment.yaml\n",
      "        │   ├── [140K]  config.yaml\n",
      "        │   ├── [4.7M]  output.log\n",
      "        │   ├── [5.4K]  requirements.txt\n",
      "        │   ├── [2.1K]  wandb-metadata.json\n",
      "        │   └── [653K]  wandb-summary.json\n",
      "        ├── [4.0K]  logs\n",
      "        │   ├── [3.4M]  debug-internal.log\n",
      "        │   └── [8.2K]  debug.log\n",
      "        └── [113M]  run-1g372i3v.wandb\n",
      "\n",
      "9 directories, 34 files\n"
     ]
    }
   ],
   "source": [
    "!tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the *5-gram* LM is quite large - it amounts to more than 4 GB.\n",
    "To reduce the size of the *n-gram* and make loading faster, `kenLM` allows converting `.arpa` files to binary ones using the `build_binary` executable.\n",
    "\n",
    "Let's make use of it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading xls-r-300m-sv/language_model/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/build_binary xls-r-300m-sv/language_model/5gram_correct.arpa xls-r-300m-sv/language_model/5gram.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it worked! Let's remove the `.arpa` file and check the size of the binary *5-gram* LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xls-r-300m-sv/\n",
      "├── [  23]  added_tokens.json\n",
      "├── [ 401]  all_results.json\n",
      "├── [ 253]  alphabet.json\n",
      "├── [2.0K]  config.json\n",
      "├── [ 304]  emissions.csv\n",
      "├── [ 226]  eval_results.json\n",
      "├── [4.0K]  language_model\n",
      "│   ├── [1.8G]  5gram.bin\n",
      "│   ├── [  78]  attrs.json\n",
      "│   └── [4.9M]  unigrams.txt\n",
      "├── [ 240]  preprocessor_config.json\n",
      "├── [1.2G]  pytorch_model.bin\n",
      "├── [3.5K]  README.md\n",
      "├── [4.0K]  runs\n",
      "│   └── [4.0K]  Jan09_22-00-50_brutasse\n",
      "│       ├── [4.0K]  1641765760.8871996\n",
      "│       │   └── [4.6K]  events.out.tfevents.1641765760.brutasse.31164.1\n",
      "│       ├── [ 42K]  events.out.tfevents.1641765760.brutasse.31164.0\n",
      "│       └── [ 364]  events.out.tfevents.1641794162.brutasse.31164.2\n",
      "├── [1.2K]  run.sh\n",
      "├── [ 30K]  run_speech_recognition_ctc.py\n",
      "├── [ 502]  special_tokens_map.json\n",
      "├── [ 279]  tokenizer_config.json\n",
      "├── [ 29K]  trainer_state.json\n",
      "├── [2.9K]  training_args.bin\n",
      "├── [ 196]  train_results.json\n",
      "├── [ 319]  vocab.json\n",
      "└── [4.0K]  wandb\n",
      "    ├── [  52]  debug-internal.log -> run-20220109_220240-1g372i3v/logs/debug-internal.log\n",
      "    ├── [  43]  debug.log -> run-20220109_220240-1g372i3v/logs/debug.log\n",
      "    ├── [  28]  latest-run -> run-20220109_220240-1g372i3v\n",
      "    └── [4.0K]  run-20220109_220240-1g372i3v\n",
      "        ├── [4.0K]  files\n",
      "        │   ├── [8.8K]  conda-environment.yaml\n",
      "        │   ├── [140K]  config.yaml\n",
      "        │   ├── [4.7M]  output.log\n",
      "        │   ├── [5.4K]  requirements.txt\n",
      "        │   ├── [2.1K]  wandb-metadata.json\n",
      "        │   └── [653K]  wandb-summary.json\n",
      "        ├── [4.0K]  logs\n",
      "        │   ├── [3.4M]  debug-internal.log\n",
      "        │   └── [8.2K]  debug.log\n",
      "        └── [113M]  run-1g372i3v.wandb\n",
      "\n",
      "9 directories, 34 files\n"
     ]
    }
   ],
   "source": [
    "!rm xls-r-300m-sv/language_model/5gram_correct.arpa && tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we reduced the *n-gram* by more than half to less than 2GB now. In the final step, let's upload all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS: (1 of 1 files) 1.85 GB / 1.85 GB\n",
      "Counting objects: 9, done.\n",
      "Delta compression using up to 2 threads.\n",
      "Compressing objects: 100% (9/9), done.\n",
      "Writing objects: 100% (9/9), 1.23 MiB | 1.92 MiB/s, done.\n",
      "Total 9 (delta 3), reused 0 (delta 0)\n",
      "To https://huggingface.co/hf-test/xls-r-300m-sv\n",
      "   27d0c57..5a191e2  main -> main\n"
     ]
    }
   ],
   "source": [
    "repo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Now you should be able to use the *5gram* for LM-boosted decoding as shown in Section 1.\n",
    "\n",
    "As can be seen on [`xls-r-300m-sv`'s model card](https://huggingface.co/hf-test/xls-r-300m-sv#inference-with-lm) our *5gram* LM-boosted decoder yields a WER of 18.85% on Common Voice's 7 test set which is a relative performance of *ca.* 30% 🔥."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
