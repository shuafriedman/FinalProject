{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libboost-program-options-dev is already the newest version (1.71.0.0ubuntu2).\n",
      "libboost-system-dev is already the newest version (1.71.0.0ubuntu2).\n",
      "libboost-thread-dev is already the newest version (1.71.0.0ubuntu2).\n",
      "libbz2-dev is already the newest version (1.0.8-2).\n",
      "libboost-test-dev is already the newest version (1.71.0.0ubuntu2).\n",
      "libeigen3-dev is already the newest version (3.3.7-2).\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\n",
      "cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n",
      "liblzma-dev is already the newest version (5.2.4-1ubuntu1.1).\n",
      "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu1.5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before downloading and unpacking the KenLM repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-16 08:38:57--  https://kheafield.com/code/kenlm.tar.gz\n",
      "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
      "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 491888 (480K) [application/x-gzip]\n",
      "Saving to: ‚ÄòSTDOUT‚Äô\n",
      "\n",
      "-                     0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-                   100%[===================>] 480.36K  1.33MB/s    in 0.4s    \n",
      "\n",
      "2024-02-16 08:38:57 (1.33 MB/s) - written to stdout [491888/491888]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KenLM is written in C++, so we'll make use of `cmake` to build the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòkenlm/build‚Äô: File exists\n",
      "build_binary  fragment\t       lmplz\t\t\t     query\n",
      "count_ngrams  interpolate      phrase_table_vocab\t     streaming_example\n",
      "filter\t      kenlm_benchmark  probing_hash_table_benchmark\n"
     ]
    }
   ],
   "source": [
    "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
    "!ls kenlm/build/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyctcdecode\n",
    "!pip install kenlm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, as we can see, the executable functions have successfully been built under `kenlm/build/bin/`.\n",
    "\n",
    "KenLM by default computes an *n-gram* with [Kneser-Ney smooting](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing). All text data used to create the *n-gram* is expected to be stored in a text file.\n",
    "We download our dataset and save it as a `.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just have to run KenLM's `lmplz` command to build our *n-gram*, called `\"5gram.arpa\"`. As it's relatively common in speech recognition, we build a *5-gram* by passing the `-o 5` parameter.\n",
    "For more information on the different *n-gram* LM that can be built\n",
    "with KenLM, one can take a look at the [official website of KenLM](https://kheafield.com/code/kenlm/).\n",
    "\n",
    "Executing the command below might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /teamspace/studios/this_studio/FinalProject/word2vec-kenlm/dafyomi_train.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Unigram tokens 731986 types 34680\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:416160 2:1284321408 3:2408102912 4:3852964352 5:5618906624\n",
      "Statistics:\n",
      "1 34680 D1=0.593097 D2=1.08412 D3+=1.51264\n",
      "2 312019 D1=0.77261 D2=1.18637 D3+=1.46538\n",
      "3 569477 D1=0.88998 D2=1.33943 D3+=1.46435\n",
      "4 669576 D1=0.954196 D2=1.49286 D3+=1.66413\n",
      "5 697205 D1=0.965374 D2=1.63683 D3+=1.65578\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 48 assuming -p 1.5\n",
      "probing 57 assuming -r models -p 1.5\n",
      "trie    23 without quantization\n",
      "trie    12 assuming -q 8 -b 8 quantization \n",
      "trie    20 assuming -a 22 array pointer compression\n",
      "trie    10 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:416160 2:4992304 3:11389540 4:16069824 5:19521740\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:416160 2:4992304 3:11389540 4:16069824 5:19521740\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13028416 kB\tVmRSS:7784 kB\tRSSMax:2312332 kB\tuser:2.87346\tsys:2.00577\tCPU:4.87928\treal:7.70819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!kenlm/build/bin/lmplz -o 5 <\"dafyomi_train.txt\" > \"5gram.arpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have built a *5-gram* LM! Let's inspect the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=34680\n",
      "ngram 2=312019\n",
      "ngram 3=569477\n",
      "ngram 4=669576\n",
      "ngram 5=697205\n",
      "\n",
      "\\1-grams:\n",
      "-5.485567\t<unk>\t0\n",
      "0\t<s>\t-0.9444685\n",
      "-2.1628535\t</s>\t0\n",
      "-3.8975027\t◊©◊ú◊ï◊ù\t-0.14351521\n",
      "-3.845749\t◊ú◊õ◊ï◊ú◊ù\t-0.17207906\n",
      "-2.6802616\t◊ê◊†◊ó◊†◊ï\t-0.6201084\n",
      "-3.3024302\t◊ú◊ï◊û◊ì◊ô◊ù\t-0.37537387\n",
      "-1.9505466\t◊ê◊™\t-0.64870334\n",
      "-4.087515\t◊ì◊£\t-0.16991249\n",
      "-4.4726863\t◊õ◊ï\t-0.2208741\n",
      "-3.3253672\t◊ë◊û◊°◊õ◊™\t-0.8299429\n",
      "-3.799511\t◊®◊ê◊©\t-0.43018007\n"
     ]
    }
   ],
   "source": [
    "!head -20 5gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small problem that ü§ó Transformers will not be happy about later on.\n",
    "The *5-gram* correctly includes a \"Unknown\" or `<unk>`, as well as a *begin-of-sentence*, `<s>` token, but no *end-of-sentence*, `</s>` token.\n",
    "This sadly has to be corrected currently after the build.\n",
    "\n",
    "We can simply add the *end-of-sentence* token by adding the line `0 </s>  -0.11831701` below the *begin-of-sentence* token and increasing the `ngram 1` count by 1. Because the file has roughly 100 million lines, this command will take *ca.* 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the corrected *5-gram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=34681\n",
      "ngram 2=312019\n",
      "ngram 3=569477\n",
      "ngram 4=669576\n",
      "ngram 5=697205\n",
      "\n",
      "\\1-grams:\n",
      "-5.485567\t<unk>\t0\n",
      "0\t<s>\t-0.9444685\n",
      "0\t</s>\t-0.9444685\n",
      "-2.1628535\t</s>\t0\n",
      "-3.8975027\t◊©◊ú◊ï◊ù\t-0.14351521\n",
      "-3.845749\t◊ú◊õ◊ï◊ú◊ù\t-0.17207906\n",
      "-2.6802616\t◊ê◊†◊ó◊†◊ï\t-0.6201084\n",
      "-3.3024302\t◊ú◊ï◊û◊ì◊ô◊ù\t-0.37537387\n",
      "-1.9505466\t◊ê◊™\t-0.64870334\n",
      "-4.087515\t◊ì◊£\t-0.16991249\n",
      "-4.4726863\t◊õ◊ï\t-0.2208741\n",
      "-3.3253672\t◊ë◊û◊°◊õ◊™\t-0.8299429\n"
     ]
    }
   ],
   "source": [
    "!head -20 5gram_correct.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this looks better! We're done at this point and all that is left to do is to correctly integrate the `\"ngram\"` with [`pyctcdecode`](https://github.com/kensho-technologies/pyctcdecode) and ü§ó Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b66aa0c37f04653af388a46e9c9a67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800c1f07c7ea4a5f8797acee3e952384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a231a332f7704c338cb21ea40ea7ef29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8522d88166e46c283c7b014be9b79be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37f70b3e6f34f79a7bff5e260524f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5bc51cc5d846878252532a81633cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the vocabulary of its tokenizer as it represents the `\"labels\"` of `pyctcdecode`'s `BeamSearchDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"labels\"` and the previously built `5gram_correct.arpa` file is all that's needed to build the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/FinalProject/word2vec-kenlm/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely ignore the warning and all that is left to do now is to wrap the just created `decoder`, together with the processor's `tokenizer` and `feature_extractor` into a `Wav2Vec2ProcessorWithLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to directly upload the LM-boosted processor into\n",
    "the model folder of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv) to have all relevant files in one place.\n",
    "\n",
    "Let's clone the repo, add the new decoder files and upload them afterward.\n",
    "First, we need to install `git-lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install git-lfs tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning and uploading of modeling files can be done conveniently with the `huggingface_hub`'s `Repository` class.\n",
    "\n",
    "More information on how to use the `huggingface_hub` to upload any files, please take a look at the [official docs](https://huggingface.co/docs/hub/how-to-upstream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import Repository\n",
    "\n",
    "# repo = Repository(local_dir=\"xls-r-300m-sv\", clone_from=\"hf-test/xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cloned `xls-r-300m-sv`, let's save the new processor with LM into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor_with_lm.save_pretrained(\"xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the local repository. The `tree` command conveniently can also show the size of the different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the *5-gram* LM is quite large - it amounts to more than 4 GB.\n",
    "To reduce the size of the *n-gram* and make loading faster, `kenLM` allows converting `.arpa` files to binary ones using the `build_binary` executable.\n",
    "\n",
    "Let's make use of it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kenlm/build/bin/build_binary xls-r-300m-sv/language_model/5gram_correct.arpa xls-r-300m-sv/language_model/5gram.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it worked! Let's remove the `.arpa` file and check the size of the binary *5-gram* LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm xls-r-300m-sv/language_model/5gram_correct.arpa && tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we reduced the *n-gram* by more than half to less than 2GB now. In the final step, let's upload all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the processor to local\n",
    "processor_with_lm.save_pretrained(\"/teamspace/studios/this_studio/FinalProject/models/KenLM-Wav2Vec2-Hebrew\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
