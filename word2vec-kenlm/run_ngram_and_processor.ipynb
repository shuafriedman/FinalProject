{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd() +'/data/mesivta/'\n",
    "file = path + 'train.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before downloading and unpacking the KenLM repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-03 19:36:41--  https://kheafield.com/code/kenlm.tar.gz\n",
      "Resolving kheafield.com (kheafield.com)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.196.63.85\n",
      "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 491888 (480K) [application/x-gzip]\n",
      "Saving to: ‚ÄòSTDOUT‚Äô\n",
      "\n",
      "-                   100%[===================>] 480.36K   340KB/s    in 1.4s    \n",
      "\n",
      "2024-03-03 19:36:42 (340 KB/s) - written to stdout [491888/491888]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "if platform.system() == 'Darwin':  # Darwin stands for MacOS\n",
    "    !curl -L https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
    "elif platform.system() == 'Linux':\n",
    "    !wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
    "else:\n",
    "    print(\"Unsupported operating system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KenLM is written in C++, so we'll make use of `cmake` to build the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòkenlm/build‚Äô: File exists\n",
      "build_binary  fragment\t       lmplz\t\t\t     query\n",
      "count_ngrams  interpolate      phrase_table_vocab\t     streaming_example\n",
      "filter\t      kenlm_benchmark  probing_hash_table_benchmark\n"
     ]
    }
   ],
   "source": [
    "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
    "!ls kenlm/build/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyctcdecode in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyctcdecode) (1.26.2)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyctcdecode) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyctcdecode) (6.97.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: kenlm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode\n",
    "!pip install kenlm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, as we can see, the executable functions have successfully been built under `kenlm/build/bin/`.\n",
    "\n",
    "KenLM by default computes an *n-gram* with [Kneser-Ney smooting](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing). All text data used to create the *n-gram* is expected to be stored in a text file.\n",
    "We download our dataset and save it as a `.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just have to run KenLM's `lmplz` command to build our *n-gram*, called `\"5gram.arpa\"`. As it's relatively common in speech recognition, we build a *5-gram* by passing the `-o 5` parameter.\n",
    "For more information on the different *n-gram* LM that can be built\n",
    "with KenLM, one can take a look at the [official website of KenLM](https://kheafield.com/code/kenlm/).\n",
    "\n",
    "Executing the command below might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /teamspace/studios/this_studio/FinalProject/word2vec-kenlm/data/mesivta/train.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Unigram tokens 4989721 types 161274\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1935288 2:1270423680 3:2382044672 4:3811271168 5:5558104064\n",
      "Statistics:\n",
      "1 161274 D1=0.592941 D2=1.05761 D3+=1.50101\n",
      "2 2136990 D1=0.784366 D2=1.17542 D3+=1.47568\n",
      "3 3902755 D1=0.90099 D2=1.34268 D3+=1.58097\n",
      "4 4541403 D1=0.956203 D2=1.50116 D3+=1.68527\n",
      "5 4760756 D1=0.95573 D2=1.51904 D3+=1.65912\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 327 assuming -p 1.5\n",
      "probing 389 assuming -r models -p 1.5\n",
      "trie    162 without quantization\n",
      "trie     90 assuming -q 8 -b 8 quantization \n",
      "trie    142 assuming -a 22 array pointer compression\n",
      "trie     70 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:1935288 2:34191840 3:78055100 4:108993672 5:133301168\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:1935288 2:34191840 3:78055100 4:108993672 5:133301168\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:12890788 kB\tVmRSS:10444 kB\tRSSMax:2399336 kB\tuser:17.4722\tsys:5.71816\tCPU:23.1904\treal:20.8515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!kenlm/build/bin/lmplz -o 5 <\"{file}\" > \"{path}/5gram.arpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have built a *5-gram* LM! Let's inspect the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=161274\n",
      "ngram 2=2136990\n",
      "ngram 3=3902755\n",
      "ngram 4=4541403\n",
      "ngram 5=4760756\n",
      "\n",
      "\\1-grams:\n",
      "-6.3179564\t<unk>\t0\n",
      "0\t<s>\t-0.59340286\n",
      "-2.8985147\t</s>\t0\n",
      "-2.965528\t◊ï◊õ◊ü\t-0.4433093\n",
      "-3.892803\t◊§◊®◊ò\t-0.2773799\n",
      "-6.173038\t◊ú◊û◊°◊ï◊ú◊ô◊ù\t-0.10548098\n",
      "-2.976362\t◊©◊î◊ô◊ê\t-0.44526786\n",
      "-4.6349635\t◊†◊¢◊ú\t-0.17592013\n",
      "-2.783812\t◊©◊ê◊ô◊ü\t-0.5445806\n",
      "-2.757102\t◊ú◊ï\t-0.46461868\n",
      "-4.3451552\t◊¢◊ß◊ë\t-0.15198769\n",
      "-3.0338998\t◊©◊ê◊ô◊†◊ï\t-0.60274816\n"
     ]
    }
   ],
   "source": [
    "!head -20 {path}5gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small problem that ü§ó Transformers will not be happy about later on.\n",
    "The *5-gram* correctly includes a \"Unknown\" or `<unk>`, as well as a *begin-of-sentence*, `<s>` token, but no *end-of-sentence*, `</s>` token.\n",
    "This sadly has to be corrected currently after the build.\n",
    "\n",
    "We can simply add the *end-of-sentence* token by adding the line `0 </s>  -0.11831701` below the *begin-of-sentence* token and increasing the `ngram 1` count by 1. Because the file has roughly 100 million lines, this command will take *ca.* 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}5gram.arpa\", \"r\") as read_file, open(f\"{path}5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the corrected *5-gram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=161275\n",
      "ngram 2=2136990\n",
      "ngram 3=3902755\n",
      "ngram 4=4541403\n",
      "ngram 5=4760756\n",
      "\n",
      "\\1-grams:\n",
      "-6.3179564\t<unk>\t0\n",
      "0\t<s>\t-0.59340286\n",
      "0\t</s>\t-0.59340286\n",
      "-2.8985147\t</s>\t0\n",
      "-2.965528\t◊ï◊õ◊ü\t-0.4433093\n",
      "-3.892803\t◊§◊®◊ò\t-0.2773799\n",
      "-6.173038\t◊ú◊û◊°◊ï◊ú◊ô◊ù\t-0.10548098\n",
      "-2.976362\t◊©◊î◊ô◊ê\t-0.44526786\n",
      "-4.6349635\t◊†◊¢◊ú\t-0.17592013\n",
      "-2.783812\t◊©◊ê◊ô◊ü\t-0.5445806\n",
      "-2.757102\t◊ú◊ï\t-0.46461868\n",
      "-4.3451552\t◊¢◊ß◊ë\t-0.15198769\n"
     ]
    }
   ],
   "source": [
    "!head -20 {path}5gram_correct.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this looks better! We're done at this point and all that is left to do is to correctly integrate the `\"ngram\"` with [`pyctcdecode`](https://github.com/kensho-technologies/pyctcdecode) and ü§ó Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the vocabulary of its tokenizer as it represents the `\"labels\"` of `pyctcdecode`'s `BeamSearchDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"labels\"` and the previously built `5gram_correct.arpa` file is all that's needed to build the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/FinalProject/word2vec-kenlm/data/mesivta/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=f\"{path}5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely ignore the warning and all that is left to do now is to wrap the just created `decoder`, together with the processor's `tokenizer` and `feature_extractor` into a `Wav2Vec2ProcessorWithLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to directly upload the LM-boosted processor into\n",
    "the model folder of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv) to have all relevant files in one place.\n",
    "\n",
    "Let's clone the repo, add the new decoder files and upload them afterward.\n",
    "First, we need to install `git-lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (2.9.2-1).\n",
      "The following NEW packages will be installed:\n",
      "  tree\n",
      "0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 43.0 kB of archives.\n",
      "After this operation, 115 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
      "Fetched 43.0 kB in 0s (295 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package tree.\n",
      "(Reading database ... 82245 files and directories currently installed.)\n",
      "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
      "Unpacking tree (1.8.0-1) ...\n",
      "Setting up tree (1.8.0-1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning and uploading of modeling files can be done conveniently with the `huggingface_hub`'s `Repository` class.\n",
    "\n",
    "More information on how to use the `huggingface_hub` to upload any files, please take a look at the [official docs](https://huggingface.co/docs/hub/how-to-upstream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/imvladikon/wav2vec2-xls-r-300m-hebrew into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a5fe324339495aa35825f729e77a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 32.0k/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8eaaf87e50419cb04c5e2d6eede3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 2.98k/2.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87558133ef449d092728aa766c5b8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  34%|###3      | 1.00k/2.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bee8df986b1472da4431ed320e0d433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file model.safetensors:   0%|          | 1.40k/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b4d13c02814d4d9796b327ff6a1786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e2db8538614cf89daaffa6bd8f5c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file model.safetensors:   0%|          | 1.00k/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import Repository\n",
    "\n",
    "# repo = Repository(local_dir=\"FinalProject/models/ken-lm\", clone_from=\"imvladikon/wav2vec2-xls-r-300m-hebrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cloned `xls-r-300m-sv`, let's save the new processor with LM into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor_with_lm.save_pretrained(\"xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the local repository. The `tree` command conveniently can also show the size of the different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the *5-gram* LM is quite large - it amounts to more than 4 GB.\n",
    "To reduce the size of the *n-gram* and make loading faster, `kenLM` allows converting `.arpa` files to binary ones using the `build_binary` executable.\n",
    "\n",
    "Let's make use of it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kenlm/build/bin/build_binary xls-r-300m-sv/language_model/5gram_correct.arpa xls-r-300m-sv/language_model/5gram.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it worked! Let's remove the `.arpa` file and check the size of the binary *5-gram* LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm xls-r-300m-sv/language_model/5gram_correct.arpa && tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we reduced the *n-gram* by more than half to less than 2GB now. In the final step, let's upload all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the processor to local\n",
    "processor_with_lm.save_pretrained(\"/teamspace/studios/this_studio/FinalProject/models/KenLM-Wav2Vec2-Hebrew-Mesivta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
