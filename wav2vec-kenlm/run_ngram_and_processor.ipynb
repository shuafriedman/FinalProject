{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd() +'/FinalProject/wav2vec-kenlm/data/ivritai/'\n",
    "file = path + 'ivritai.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before downloading and unpacking the KenLM repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-30 17:29:59--  https://kheafield.com/code/kenlm.tar.gz\n",
      "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
      "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 491888 (480K) [application/x-gzip]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                     0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-                   100%[===================>] 480.36K  1.77MB/s    in 0.3s    \n",
      "\n",
      "2024-04-30 17:29:59 (1.77 MB/s) - written to stdout [491888/491888]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "if platform.system() == 'Darwin':  # Darwin stands for MacOS\n",
    "    !curl -L https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
    "elif platform.system() == 'Linux':\n",
    "    !wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
    "else:\n",
    "    print(\"Unsupported operating system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KenLM is written in C++, so we'll make use of `cmake` to build the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘kenlm/build’: File exists\n",
      "build_binary  fragment\t       lmplz\t\t\t     query\n",
      "count_ngrams  interpolate      phrase_table_vocab\t     streaming_example\n",
      "filter\t      kenlm_benchmark  probing_hash_table_benchmark\n"
     ]
    }
   ],
   "source": [
    "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
    "!ls kenlm/build/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyctcdecode in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyctcdecode) (1.23.0)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyctcdecode) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyctcdecode) (6.97.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: kenlm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode\n",
    "!pip install kenlm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, as we can see, the executable functions have successfully been built under `kenlm/build/bin/`.\n",
    "\n",
    "KenLM by default computes an *n-gram* with [Kneser-Ney smooting](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing). All text data used to create the *n-gram* is expected to be stored in a text file.\n",
    "We download our dataset and save it as a `.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just have to run KenLM's `lmplz` command to build our *n-gram*, called `\"5gram.arpa\"`. As it's relatively common in speech recognition, we build a *5-gram* by passing the `-o 5` parameter.\n",
    "For more information on the different *n-gram* LM that can be built\n",
    "with KenLM, one can take a look at the [official website of KenLM](https://kheafield.com/code/kenlm/).\n",
    "\n",
    "Executing the command below might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/data/ivritai/ivritai.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Unigram tokens 666793 types 68410\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:820920 2:1270531840 3:2382247168 4:3811595520 5:5558577152\n",
      "Statistics:\n",
      "1 68410 D1=0.676854 D2=1.07847 D3+=1.38516\n",
      "2 386701 D1=0.846211 D2=1.16431 D3+=1.33312\n",
      "3 576613 D1=0.94022 D2=1.29629 D3+=1.40398\n",
      "4 593907 D1=0.980639 D2=1.48502 D3+=1.6486\n",
      "5 557180 D1=0.971796 D2=1.8707 D3+=1.29316\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 46 assuming -p 1.5\n",
      "probing 56 assuming -r models -p 1.5\n",
      "trie    23 without quantization\n",
      "trie    13 assuming -q 8 -b 8 quantization \n",
      "trie    20 assuming -a 22 array pointer compression\n",
      "trie    10 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:820920 2:6187216 3:11532260 4:14253768 5:15601040\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:820920 2:6187216 3:11532260 4:14253768 5:15601040\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:12890788 kB\tVmRSS:8760 kB\tRSSMax:2287108 kB\tuser:2.67389\tsys:1.95244\tCPU:4.62638\treal:4.22001\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 <\"{file}\" > \"{path}/5gram.arpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have built a *5-gram* LM! Let's inspect the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=68410\n",
      "ngram 2=386701\n",
      "ngram 3=576613\n",
      "ngram 4=593907\n",
      "ngram 5=557180\n",
      "\n",
      "\\1-grams:\n",
      "-5.625689\t<unk>\t0\n",
      "0\t<s>\t-0.64655524\n",
      "-1.3858544\t</s>\t0\n",
      "-2.838002\tאה\t-0.27534404\n",
      "-3.1600022\tנגיד\t-0.23054226\n",
      "-2.3611543\tאתה\t-0.6647856\n",
      "-3.524679\tיודע\t-0.21384771\n",
      "-1.9815823\tזה\t-0.49287868\n",
      "-2.8288188\tכזה\t-0.30056417\n",
      "-4.772435\tימכרו\t-0.0725213\n",
      "-1.943969\tאת\t-0.45797426\n",
      "-3.1616285\tה\t-0.4458315\n"
     ]
    }
   ],
   "source": [
    "!head -20 {path}5gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small problem that 🤗 Transformers will not be happy about later on.\n",
    "The *5-gram* correctly includes a \"Unknown\" or `<unk>`, as well as a *begin-of-sentence*, `<s>` token, but no *end-of-sentence*, `</s>` token.\n",
    "This sadly has to be corrected currently after the build.\n",
    "\n",
    "We can simply add the *end-of-sentence* token by adding the line `0 </s>  -0.11831701` below the *begin-of-sentence* token and increasing the `ngram 1` count by 1. Because the file has roughly 100 million lines, this command will take *ca.* 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}5gram.arpa\", \"r\") as read_file, open(f\"{path}5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the corrected *5-gram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=68411\n",
      "ngram 2=386701\n",
      "ngram 3=576613\n",
      "ngram 4=593907\n",
      "ngram 5=557180\n",
      "\n",
      "\\1-grams:\n",
      "-5.625689\t<unk>\t0\n",
      "0\t<s>\t-0.64655524\n",
      "0\t</s>\t-0.64655524\n",
      "-1.3858544\t</s>\t0\n",
      "-2.838002\tאה\t-0.27534404\n",
      "-3.1600022\tנגיד\t-0.23054226\n",
      "-2.3611543\tאתה\t-0.6647856\n",
      "-3.524679\tיודע\t-0.21384771\n",
      "-1.9815823\tזה\t-0.49287868\n",
      "-2.8288188\tכזה\t-0.30056417\n",
      "-4.772435\tימכרו\t-0.0725213\n",
      "-1.943969\tאת\t-0.45797426\n"
     ]
    }
   ],
   "source": [
    "!head -20 {path}5gram_correct.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this looks better! We're done at this point and all that is left to do is to correctly integrate the `\"ngram\"` with [`pyctcdecode`](https://github.com/kensho-technologies/pyctcdecode) and 🤗 Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor\n",
    "from transformers import AutoProcessor, Wav2Vec2BertProcessor\n",
    "# processor = AutoProcessor.from_pretrained(\"imvladikon/wav2vec2-xls-r-300m-hebrew\",\n",
    "#                                             unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    "# )\n",
    "processor=Wav2Vec2BertProcessor.from_pretrained(\"/teamspace/studios/this_studio/models/facebook/w2v-bert-2.0-finetuned\", \n",
    "                                            unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2CTCTokenizer(name_or_path='/teamspace/studios/this_studio/models/facebook/w2v-bert-2.0-finetuned', vocab_size=34, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t30: AddedToken(\"[UNK]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t31: AddedToken(\"[PAD]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t32: AddedToken(\"<s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t33: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the vocabulary of its tokenizer as it represents the `\"labels\"` of `pyctcdecode`'s `BeamSearchDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"labels\"` and the previously built `5gram_correct.arpa` file is all that's needed to build the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/data/ivritai/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=f\"{path}5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely ignore the warning and all that is left to do now is to wrap the just created `decoder`, together with the processor's `tokenizer` and `feature_extractor` into a `Wav2Vec2ProcessorWithLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to directly upload the LM-boosted processor into\n",
    "the model folder of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv) to have all relevant files in one place.\n",
    "\n",
    "Let's clone the repo, add the new decoder files and upload them afterward.\n",
    "First, we need to install `git-lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install git-lfs tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning and uploading of modeling files can be done conveniently with the `huggingface_hub`'s `Repository` class.\n",
    "\n",
    "More information on how to use the `huggingface_hub` to upload any files, please take a look at the [official docs](https://huggingface.co/docs/hub/how-to-upstream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import Repository\n",
    "\n",
    "# repo = Repository(local_dir=\"FinalProject/models/ken-lm\", clone_from=\"imvladikon/wav2vec2-xls-r-300m-hebrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cloned `xls-r-300m-sv`, let's save the new processor with LM into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor_with_lm.save_pretrained(\"xls-r-300m-sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the local repository. The `tree` command conveniently can also show the size of the different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the *5-gram* LM is quite large - it amounts to more than 4 GB.\n",
    "To reduce the size of the *n-gram* and make loading faster, `kenLM` allows converting `.arpa` files to binary ones using the `build_binary` executable.\n",
    "\n",
    "Let's make use of it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kenlm/build/bin/build_binary xls-r-300m-sv/language_model/5gram_correct.arpa xls-r-300m-sv/language_model/5gram.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it worked! Let's remove the `.arpa` file and check the size of the binary *5-gram* LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm xls-r-300m-sv/language_model/5gram_correct.arpa && tree -h xls-r-300m-sv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we reduced the *n-gram* by more than half to less than 2GB now. In the final step, let's upload all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the processor to local\n",
    "# processor_with_lm.save_pretrained(\"/teamspace/studios/this_studio/models/KenLM-Wav2Vec2-imvladikon-300m-ivritai\")\n",
    "processor_with_lm.save_pretrained(\"/teamspace/studios/this_studio/models/wav2vec2bert-ivritai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
