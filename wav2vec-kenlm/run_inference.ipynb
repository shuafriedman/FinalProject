{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Combine an *n-gram* with Wav2Vec2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we want to wrap the *5-gram* into a `Wav2Vec2ProcessorWithLM` object to make the *5-gram* boosted decoding as seamless as shown in Section 1.\n",
    "We start by downloading the currently \"LM-less\" processor of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... 0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run inference on test dataset first example\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.25.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first 10 seconds of the audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "import math\n",
    "\n",
    "# Load your MP3 file\n",
    "audio = AudioSegment.from_mp3(\"FinalProject/wav2vec-kenlm/data/dafyomi/batra_155.mp3\")\n",
    "\n",
    "# Define the length of each chunk in milliseconds\n",
    "chunk_length_ms = 10000  # 10 seconds * 1000 ms/sec\n",
    "chunks = make_chunks(audio, chunk_length_ms) \n",
    "chunks = [chunk.set_frame_rate(16000).set_channels(1) for chunk in chunks]\n",
    "chunks = [np.array(chunk.get_array_of_samples()) for chunk in chunks]\n",
    "chunks = [chunk.astype(np.float32) / np.abs(chunk).max() for chunk in chunks]\n",
    "# Calculate the number of chunks to split the file into\n",
    "# num_chunks = math.ceil(len(audio) / chunk_length_ms)\n",
    "# chunks = []\n",
    "# Split the audio and save each chunk\n",
    "# for i in range(num_chunks):\n",
    "#     start_ms = i * chunk_length_ms\n",
    "#     end_ms = min((i + 1) * chunk_length_ms, len(audio))\n",
    "#     chunk = audio[start_ms:end_ms]\n",
    "#     chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "class ASRModel:\n",
    "    def __init__(self, model_name=None, model=None, processor=None, feature_extractor=None, tokenizer=None, lm_model=False):\n",
    "        self.model_name=model_name\n",
    "        self.feature_extractor=feature_extractor\n",
    "        self.processor=processor\n",
    "        self.tokenizer=tokenizer\n",
    "        self.lm_model=lm_model\n",
    "        if feature_extractor and tokenizer:\n",
    "            self.feature_extractor=feature_extractor\n",
    "            self.tokenizer=tokenizer\n",
    "            self.processor=AutoProcessor(feature_extractor=feature_extractor, processor=processor)\n",
    "\n",
    "        elif processor:\n",
    "            self.processor=processor\n",
    "        else:\n",
    "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        print('Getting Model...')\n",
    "        if lm_model:\n",
    "            self.model= AutoModelForCTC.from_pretrained(model_name)\n",
    "        elif model:\n",
    "           self.model=model \n",
    "        else:\n",
    "            self.model = AutoModelForCTC.from_pretrained(model_name)\n",
    "\n",
    "            \n",
    "    def get_prediction(self, inputs, sampling_rate=16000, return_tensors=\"pt\"):\n",
    "        self.inputs= self.processor(inputs, sampling_rate=sampling_rate, return_tensors=return_tensors)\n",
    "        with torch.no_grad():\n",
    "            self.logits = self.model(**self.inputs).logits\n",
    "        if self.lm_model:\n",
    "            return self.lm_model.batch_decode(self.logits.numpy()).text\n",
    "        else:\n",
    "            predicted_ids = torch.argmax(self.logits, dim=-1)\n",
    "            return self.processor.batch_decode(predicted_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = chunks[10]\n",
    "sf.write(\"bert_test.wav\", sample, 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/models/wav2vec2bertLm/language_model/5gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, SeamlessM4TFeatureExtractor, Wav2Vec2BertForCTC, Wav2Vec2ProcessorWithLM, Wav2Vec2BertProcessor\n",
    "processor = Wav2Vec2BertProcessor.from_pretrained('models/facebook/w2v-bert-2.0-finetuned')\n",
    "bertLM = Wav2Vec2ProcessorWithLM.from_pretrained(\"models/wav2vec2bertLm\")\n",
    "wav2vec2BertLm = ASRModel(model_name=\"models/facebook/w2v-bert-2.0-finetuned\", processor=processor, lm_model=bertLM)\n",
    "# sample = chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['סריס סביבותיו והציבור <s> ויסרס סומכוס בסרסור סירוס סורסי ריסוקי סרוס יסרהב ספסירותיה סלסלה']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = processor(sample, sampling_rate=16000, return_tensors=\"pt\")\n",
    "wav2vec2BertLm.get_prediction(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "wav2vec_he = ASRModel(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")\n",
    "wav2vec_he.get_prediction(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
