{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Combine an *n-gram* with Wav2Vec2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we want to wrap the *5-gram* into a `Wav2Vec2ProcessorWithLM` object to make the *5-gram* boosted decoding as seamless as shown in Section 1.\n",
    "We start by downloading the currently \"LM-less\" processor of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run inference on test dataset first example\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.25.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first 10 seconds of the audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "import math\n",
    "\n",
    "# Load your MP3 file\n",
    "audio = AudioSegment.from_mp3(\"FinalProject/wav2vec-kenlm/data/dafyomi/batra_155.mp3\")\n",
    "\n",
    "# Define the length of each chunk in milliseconds\n",
    "chunk_length_ms = 10000  # 10 seconds * 1000 ms/sec\n",
    "chunks = make_chunks(audio, chunk_length_ms) \n",
    "chunks = [chunk.set_frame_rate(16000).set_channels(1) for chunk in chunks]\n",
    "chunks = [np.array(chunk.get_array_of_samples()) for chunk in chunks]\n",
    "chunks = [chunk.astype(np.float32) / np.abs(chunk).max() for chunk in chunks]\n",
    "# Calculate the number of chunks to split the file into\n",
    "# num_chunks = math.ceil(len(audio) / chunk_length_ms)\n",
    "# chunks = []\n",
    "# Split the audio and save each chunk\n",
    "# for i in range(num_chunks):\n",
    "#     start_ms = i * chunk_length_ms\n",
    "#     end_ms = min((i + 1) * chunk_length_ms, len(audio))\n",
    "#     chunk = audio[start_ms:end_ms]\n",
    "#     chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "class ASRModel:\n",
    "    def __init__(self, model_name=None, model=None, processor=None, feature_extractor=None, tokenizer=None, lm_model=False):\n",
    "        self.model_name=model_name\n",
    "        self.feature_extractor=feature_extractor\n",
    "        self.processor=processor\n",
    "        self.tokenizer=tokenizer\n",
    "        self.lm_model=lm_model\n",
    "        if feature_extractor and tokenizer:\n",
    "            self.feature_extractor=feature_extractor\n",
    "            self.tokenizer=tokenizer\n",
    "            self.processor=AutoProcessor(feature_extractor=feature_extractor, processor=processor)\n",
    "\n",
    "        elif processor:\n",
    "            self.processor=processor\n",
    "        else:\n",
    "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        print('Getting Model...')\n",
    "        if lm_model:\n",
    "            self.model= AutoModelForCTC.from_pretrained(model_name)\n",
    "        elif model:\n",
    "           self.model=model \n",
    "        else:\n",
    "            self.model = AutoModelForCTC.from_pretrained(model_name)\n",
    "\n",
    "            \n",
    "    def get_prediction(self, inputs, sampling_rate=16000, return_tensors=\"pt\"):\n",
    "        self.inputs= self.processor(inputs, sampling_rate=sampling_rate, return_tensors=return_tensors)\n",
    "        with torch.no_grad():\n",
    "            self.logits = self.model(**self.inputs).logits\n",
    "        if self.lm_model:\n",
    "            return self.lm_model.batch_decode(self.logits.numpy()).text\n",
    "        else:\n",
    "            predicted_ids = torch.argmax(self.logits, dim=-1)\n",
    "            return self.processor.batch_decode(predicted_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = chunks[10]\n",
    "sf.write(\"bert_test.wav\", sample, 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/models/wav2vec2bertLm/language_model/5gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, SeamlessM4TFeatureExtractor, Wav2Vec2BertForCTC, Wav2Vec2ProcessorWithLM, Wav2Vec2BertProcessor\n",
    "processor = Wav2Vec2BertProcessor.from_pretrained('models/facebook/w2v-bert-2.0-finetuned')\n",
    "bertLM = Wav2Vec2ProcessorWithLM.from_pretrained(\"models/wav2vec2bertLm\")\n",
    "wav2vec2BertLm = ASRModel(model_name=\"models/facebook/w2v-bert-2.0-finetuned\", processor=processor, lm_model=bertLM)\n",
    "# sample = chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input logits shape is (250, 33), but vocabulary is size 32. Need logits of shape: (time, vocabulary)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# inputs = processor(sample, sampling_rate=16000, return_tensors=\"pt\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m wav2vec2BertLm\u001b[39m.\u001b[39;49mget_prediction(sample)\n",
      "\u001b[1;32m/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_model:\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_model\u001b[39m.\u001b[39;49mbatch_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogits\u001b[39m.\u001b[39;49mnumpy())\u001b[39m.\u001b[39mtext\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     predicted_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:422\u001b[0m, in \u001b[0;36mWav2Vec2ProcessorWithLM.batch_decode\u001b[0;34m(self, logits, pool, num_processes, beam_width, beam_prune_logp, token_min_logp, hotwords, hotword_weight, alpha, beta, unk_score_offset, lm_score_boundary, output_word_offsets, n_best)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m# pyctcdecode\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mwith\u001b[39;00m cm:\n\u001b[0;32m--> 422\u001b[0m     decoded_beams \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mdecode_beams_batch(\n\u001b[1;32m    423\u001b[0m         pool\u001b[39m=\u001b[39;49mpool,\n\u001b[1;32m    424\u001b[0m         logits_list\u001b[39m=\u001b[39;49mlogits_list,\n\u001b[1;32m    425\u001b[0m         beam_width\u001b[39m=\u001b[39;49mbeam_width,\n\u001b[1;32m    426\u001b[0m         beam_prune_logp\u001b[39m=\u001b[39;49mbeam_prune_logp,\n\u001b[1;32m    427\u001b[0m         token_min_logp\u001b[39m=\u001b[39;49mtoken_min_logp,\n\u001b[1;32m    428\u001b[0m         hotwords\u001b[39m=\u001b[39;49mhotwords,\n\u001b[1;32m    429\u001b[0m         hotword_weight\u001b[39m=\u001b[39;49mhotword_weight,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39m# extract text and scores\u001b[39;00m\n\u001b[1;32m    433\u001b[0m batch_texts, logit_scores, lm_scores, word_offsets \u001b[39m=\u001b[39m [], [], [], []\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pyctcdecode/decoder.py:679\u001b[0m, in \u001b[0;36mBeamSearchDecoderCTC.decode_beams_batch\u001b[0;34m(self, pool, logits_list, beam_width, beam_prune_logp, token_min_logp, prune_history, hotwords, hotword_weight)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    666\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_beams_mp_safe(\n\u001b[1;32m    667\u001b[0m             logits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[39mfor\u001b[39;00m logits \u001b[39min\u001b[39;00m logits_list\n\u001b[1;32m    676\u001b[0m     ]\n\u001b[1;32m    678\u001b[0m \u001b[39mfor\u001b[39;00m logits \u001b[39min\u001b[39;00m logits_list:\n\u001b[0;32m--> 679\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_logits_dimension(logits)\n\u001b[1;32m    680\u001b[0m p_decode \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    681\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_beams_mp_safe,\n\u001b[1;32m    682\u001b[0m     beam_width\u001b[39m=\u001b[39mbeam_width,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m     hotword_weight\u001b[39m=\u001b[39mhotword_weight,\n\u001b[1;32m    688\u001b[0m )\n\u001b[1;32m    689\u001b[0m decoded_beams_list: List[List[OutputBeamMPSafe]] \u001b[39m=\u001b[39m valid_pool\u001b[39m.\u001b[39mmap(p_decode, logits_list)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pyctcdecode/decoder.py:307\u001b[0m, in \u001b[0;36mBeamSearchDecoderCTC._check_logits_dimension\u001b[0;34m(self, logits)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInput logits have \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m dimensions, but need 2: (time, vocabulary)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(logits\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[39mif\u001b[39;00m logits\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx2vocab):\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInput logits shape is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, but vocabulary is size \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNeed logits of shape: (time, vocabulary)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (logits\u001b[39m.\u001b[39mshape, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx2vocab))\n\u001b[1;32m    310\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Input logits shape is (250, 33), but vocabulary is size 32. Need logits of shape: (time, vocabulary)"
     ]
    }
   ],
   "source": [
    "# inputs = processor(sample, sampling_rate=16000, return_tensors=\"pt\")\n",
    "wav2vec2BertLm.get_prediction(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "wav2vec_he = ASRModel(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")\n",
    "wav2vec_he.get_prediction(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
