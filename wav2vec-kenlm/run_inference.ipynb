{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Combine an *n-gram* with Wav2Vec2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we want to wrap the *5-gram* into a `Wav2Vec2ProcessorWithLM` object to make the *5-gram* boosted decoding as seamless as shown in Section 1.\n",
    "We start by downloading the currently \"LM-less\" processor of [`xls-r-300m-sv`](https://huggingface.co/hf-test/xls-r-300m-sv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run inference on test dataset first example\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.25.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first 10 seconds of the audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "import math\n",
    "\n",
    "# Load your MP3 file\n",
    "audio = AudioSegment.from_mp3(\"FinalProject/wav2vec-kenlm/data/dafyomi/batra_155.mp3\")\n",
    "\n",
    "# Define the length of each chunk in milliseconds\n",
    "chunk_length_ms = 10000  # 10 seconds * 1000 ms/sec\n",
    "chunks = make_chunks(audio, chunk_length_ms) \n",
    "chunks = [chunk.set_frame_rate(16000).set_channels(1) for chunk in chunks]\n",
    "chunks = [np.array(chunk.get_array_of_samples()) for chunk in chunks]\n",
    "chunks = [chunk.astype(np.float32) / np.abs(chunk).max() for chunk in chunks]\n",
    "# Calculate the number of chunks to split the file into\n",
    "# num_chunks = math.ceil(len(audio) / chunk_length_ms)\n",
    "# chunks = []\n",
    "# Split the audio and save each chunk\n",
    "# for i in range(num_chunks):\n",
    "#     start_ms = i * chunk_length_ms\n",
    "#     end_ms = min((i + 1) * chunk_length_ms, len(audio))\n",
    "#     chunk = audio[start_ms:end_ms]\n",
    "#     chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "class ASRModel:\n",
    "    def __init__(self, model_name=None, model=None, processor=None, feature_extractor=None, tokenizer=None, lm_model=False):\n",
    "        self.model_name=model_name\n",
    "        self.feature_extractor=feature_extractor\n",
    "        self.processor=processor\n",
    "        self.tokenizer=tokenizer\n",
    "        self.lm_model=lm_model\n",
    "        if feature_extractor and tokenizer:\n",
    "            self.feature_extractor=feature_extractor\n",
    "            self.tokenizer=tokenizer\n",
    "            self.processor=AutoProcessor(feature_extractor=feature_extractor, processor=processor)\n",
    "\n",
    "        elif processor:\n",
    "            self.processor=processor\n",
    "        else:\n",
    "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        print('Getting Model...')\n",
    "        if lm_model:\n",
    "            self.model= AutoModelForCTC.from_pretrained(model_name, vocab_size=(len(lm_model.tokenizer.vocab)))\n",
    "        elif model:\n",
    "           self.model=model \n",
    "        else:\n",
    "            self.model = AutoModelForCTC.from_pretrained(model_name)\n",
    "\n",
    "            \n",
    "    def get_prediction(self, inputs, sampling_rate=16000, return_tensors=\"pt\"):\n",
    "        self.inputs= self.processor(inputs, sampling_rate=sampling_rate, return_tensors=return_tensors)\n",
    "        with torch.no_grad():\n",
    "            self.logits = self.model(**self.inputs).logits\n",
    "        if self.lm_model:\n",
    "            return self.lm_model.batch_decode(self.logits.numpy()).text\n",
    "        else:\n",
    "            predicted_ids = torch.argmax(self.logits, dim=-1)\n",
    "            return self.processor.batch_decode(predicted_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = chunks[10]\n",
    "sf.write(\"bert_test.wav\", sample, 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/models/wav2vec2bertLm/language_model/5gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Wav2Vec2BertForCTC:\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([35, 1024]) from checkpoint, the shape in current model is torch.Size([32, 1024]).\n\tsize mismatch for lm_head.bias: copying a param with shape torch.Size([35]) from checkpoint, the shape in current model is torch.Size([32]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m processor \u001b[39m=\u001b[39m Wav2Vec2BertProcessor(feature_extractor\u001b[39m=\u001b[39mfeature_extractor, tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m bertLM \u001b[39m=\u001b[39m Wav2Vec2ProcessorWithLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmodels/wav2vec2bertLm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m wav2vec2BertLm \u001b[39m=\u001b[39m ASRModel(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels/wav2vec2Bert\u001b[39;49m\u001b[39m\"\u001b[39;49m, processor\u001b[39m=\u001b[39;49mprocessor, lm_model\u001b[39m=\u001b[39;49mbertLM)\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# sample = chunks[10]\u001b[39;00m\n",
      "\u001b[1;32m/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGetting Model...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m lm_model:\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m=\u001b[39m AutoModelForCTC\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, vocab_size\u001b[39m=\u001b[39;49m(\u001b[39mlen\u001b[39;49m(lm_model\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mvocab)))\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melif\u001b[39;00m model:\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrmwnnhcqaxrcacfvrphtn31.studio.lightning.ai/teamspace/studios/this_studio/FinalProject/wav2vec-kenlm/run_inference.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m=\u001b[39mmodel \n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3503\u001b[0m         model,\n\u001b[1;32m   3504\u001b[0m         state_dict,\n\u001b[1;32m   3505\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3507\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3508\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3509\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3510\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3511\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3512\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3513\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3514\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3515\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3516\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3517\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3518\u001b[0m     )\n\u001b[1;32m   3520\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:3977\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3973\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[1;32m   3974\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m   3975\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3976\u001b[0m         )\n\u001b[0;32m-> 3977\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unexpected_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3980\u001b[0m     archs \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39marchitectures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39marchitectures\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Wav2Vec2BertForCTC:\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([35, 1024]) from checkpoint, the shape in current model is torch.Size([32, 1024]).\n\tsize mismatch for lm_head.bias: copying a param with shape torch.Size([35]) from checkpoint, the shape in current model is torch.Size([32]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, SeamlessM4TFeatureExtractor, Wav2Vec2BertForCTC, Wav2Vec2ProcessorWithLM, Wav2Vec2BertProcessor\n",
    "tokenizer=Wav2Vec2CTCTokenizer.from_pretrained(\"models/wav2vec2bertLm\")\n",
    "feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(\"models/wav2vec2Bert\")\n",
    "processor = Wav2Vec2BertProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "bertLM = Wav2Vec2ProcessorWithLM.from_pretrained(\"models/wav2vec2bertLm\")\n",
    "wav2vec2BertLm = ASRModel(model_name=\"models/wav2vec2Bert\", processor=processor, lm_model=bertLM)\n",
    "# sample = chunks[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = processor(sample, sampling_rate=16000, return_tensors=\"pt\")\n",
    "wav2vec2BertLm.get_prediction(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "wav2vec_he = ASRModel(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")\n",
    "wav2vec_he.get_prediction(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
