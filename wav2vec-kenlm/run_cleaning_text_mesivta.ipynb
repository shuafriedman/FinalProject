{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"FinalProject/wav2vec-kenlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_to_remove = \"יעקב שולביץ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mesivta/ChavrutaShulevitz.output.txt') as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Keep only Hebrew letters, punctuation marks (.,!?,;:), and spaces\n",
    "    return re.sub(r'[^א-ת .,!?\\n]', '', text)\n",
    "\n",
    "clean_lines = [clean_text(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' כל הזכויות שמורות  לרב יעקב שולביץ   חברותא  ברכות  בלי הערות  פרק ראשון  מאימתי  הקדמה '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sections(lines):\n",
    "    sections = []\n",
    "    section=[]\n",
    "    for line in lines:\n",
    "        #if line starts with ###\n",
    "        if line.startswith('###') and section!=[]:\n",
    "            sections.append(section)\n",
    "            section = []\n",
    "            section.append(line)\n",
    "        else:\n",
    "            if line!='':\n",
    "                section.append(line)\n",
    "    sections.append(section)\n",
    "    return sections\n",
    "sections = create_sections(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(section):\n",
    "    # Split the section into sentences based on punctuation marks\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|,|!)\\s', ' '.join(section))\n",
    "    # Remove trailing punctuation and extra spaces\n",
    "    sentences = [sentence.strip(' .,!?') for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "def combine_sentences(sections):\n",
    "    combined_sentences = []\n",
    "    for section in sections:\n",
    "        sentences = split_sentences(section)\n",
    "        combined_sentences.extend(sentences)\n",
    "    return combined_sentences\n",
    "combined_sentences=combine_sentences(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_train_test(sections, percent_train=0.8):\n",
    "#     random.shuffle(sections)\n",
    "#     train = sections[:int(percent_train*len(sections))]\n",
    "#     test = sections[int(percent_train*len(sections)):]\n",
    "#     return {\"train\":train, \"test\":test}\n",
    "# dataset=split_train_test(combined_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_specific_phrase(dataset, phrase):\n",
    "    # Remove any row that contains the specified phrase\n",
    "    clean_data = [section for section in dataset if phrase not in section]\n",
    "    return clean_data\n",
    "\n",
    "def remove_headers_and_white_space(dataset):\n",
    "    #regex for all letters in between ### and ###\n",
    "    clean_data=[re.sub(r'###.*?###', '', section) for section in dataset]\n",
    "    clean_data=[re.sub(r' +', ' ', section) for section in clean_data]\n",
    "    clean_data = [section.strip() for section in clean_data]\n",
    "    return clean_data\n",
    "dataset=remove_headers_and_white_space(combined_sentences)\n",
    "\n",
    "# Apply the function to remove rows containing the specified phrase\n",
    "dataset = remove_specific_phrase(dataset, phrase_to_remove)\n",
    "# dataset['test']=remove_headers_and_white_space(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write train and test to different files\n",
    "with open('data/mesivta/mesivta_cleaned_for_kenlm.txt', 'w') as f:\n",
    "    for item in dataset:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "# with open(os.path.join(sys.path[0], 'data/mesivta/test.txt'), 'w') as f:\n",
    "#     for item in dataset['test']:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
