{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_metric, load_dataset\n",
    "from transformers import Wav2Vec2BertProcessor, Wav2Vec2BertForCTC, BatchFeature\n",
    "from transformers import Wav2Vec2CTCTokenizer, SeamlessM4TFeatureExtractor, Wav2Vec2BertForCTC, Wav2Vec2ProcessorWithLM, Wav2Vec2BertProcessor\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for mozilla-foundation/common_voice_17_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_17_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# dataset_for_wav2vec2=load_from_disk(\"/teamspace/studios/this_studio/datasets/ivritai\")\n",
    "# dataset = dataset_for_wav2vec2['test']\n",
    "# shuffled_dataset = dataset.shuffle(seed=np.random.randint(1000))\n",
    "# dataset = shuffled_dataset.select(range(1000))\n",
    "\n",
    "dataset=load_dataset(\"mozilla-foundation/common_voice_17_0\", 'he', split=\"train\")\n",
    "transcription_key_in_dictionary= 'sentence'\n",
    "shuffled_dataset = dataset.shuffle(seed=np.random.randint(1000))\n",
    "dataset = shuffled_dataset.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018d904480dd4d6583fd82d2a3583eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bb6fe64a8a4cfab29609bd606722d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c350b3e9c7cd485cb818a6b3cb13a0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c645f09b2948be97e4fb1ff25ec00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from datasets import load_metric\n",
    "# Load model and processor\n",
    "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\\%\\\\\\锟\\'\\]\\[\\{\\}\\志]'\n",
    "\n",
    "FILTER_THRESHOLD = 15\n",
    "\n",
    "# Load the dataset\n",
    "def remove_special_characters(batch):\n",
    "    batch[transcription_key_in_dictionary] = re.sub(chars_to_remove_regex, '', batch[transcription_key_in_dictionary]).lower()\n",
    "    return batch\n",
    "\n",
    "# Function to drop samples that contain English letters or digits\n",
    "def drop_english_samples(dataset):\n",
    "    def contains_english_or_digits(text):\n",
    "        english_letters = set(string.ascii_lowercase)\n",
    "        digits = set(string.digits)\n",
    "        return any(char in english_letters or char in digits for char in text.lower())\n",
    "    filtered_dataset = dataset.filter(lambda example: not contains_english_or_digits(example[transcription_key_in_dictionary]))\n",
    "    return filtered_dataset\n",
    "\n",
    "# Function to filter out long audio samples\n",
    "def filter_long_samples(dataset):\n",
    "    def is_shorter_than_max_duration(example):\n",
    "        duration_seconds = len(example['audio']['array']) / example['audio']['sampling_rate']\n",
    "        return duration_seconds <= FILTER_THRESHOLD\n",
    "    filtered_dataset = dataset.filter(lambda example: is_shorter_than_max_duration(example))\n",
    "    return filtered_dataset\n",
    "\n",
    "def remove_hebrew_vowels(batch):\n",
    "    batch[transcription_key_in_dictionary]= re.sub(\"[\\u05B0-\\u05C3\\u05C7\\u05C4]\", \"\", batch[transcription_key_in_dictionary])\n",
    "    return batch\n",
    "# Apply the functions to the dataset\n",
    "dataset = dataset.map(remove_special_characters)\n",
    "dataset = drop_english_samples(dataset)\n",
    "dataset = filter_long_samples(dataset)\n",
    "dataset = dataset.map(remove_hebrew_vowels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/wer/wer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/cer/cer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "wer = load_metric(\"wer\")\n",
    "cer = load_metric(\"cer\")\n",
    "# Setup device\n",
    "\n",
    "# Assuming the predict function is defined elsewhere and needs to be updated\n",
    "def predict(audio_tensor, model, processor=None, whisper=False):\n",
    "    # Ensure the tensor is on the correct device\n",
    "    # audio_tensor = audio_tensor.to(device)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not whisper:\n",
    "        try:\n",
    "            input_values = model.processor([audio_tensor], return_tensors=\"pt\", sampling_rate=16_000).input_features\n",
    "        except:\n",
    "            input_values = model.processor([audio_tensor], return_tensors=\"pt\", sampling_rate=16_000).input_values\n",
    "        input_values = input_values.to(device)  # Ensure inputs are on the same device as the model\n",
    "        features = input_values.to(device)\n",
    "\n",
    "        prediction = model.get_prediction(features)\n",
    "        return prediction[0]\n",
    "    else:\n",
    "        input_features = processor(audio_tensor, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        input_features = input_features.to(device)\n",
    "        predicted_ids = model.generate(input_features.input_features, language='he', num_beams=5)\n",
    "        transcript = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        return transcript[0]\n",
    "\n",
    "def evaluate_model(dataset, model, processor=None, whisper=False):\n",
    "    total_wer = 0\n",
    "    total_cer = 0\n",
    "    count = 0  # To count successfully processed examples\n",
    "    with tqdm(total=len(dataset), desc=\"Evaluating model\", unit=\" samples\") as progress_bar:\n",
    "        for example in dataset:\n",
    "            speech_array, sampling_rate = example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"]\n",
    "            # Convert numpy array to tensor and then move it to the desired device for processing\n",
    "            # speech_tensor = torch.tensor(speech_array, dtype=torch.float32).to(device)\n",
    "            # # Explicitly pass the original sampling rate and move tensor to CPU for librosa processing\n",
    "            # speech_array = speech_tensor.cpu().numpy()\n",
    "            speech_array = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)\n",
    "            # Convert the resampled array back to tensor and move to the correct device\n",
    "            predicted_text= predict(speech_array, model, processor, whisper=whisper)\n",
    "            # Compute WER\n",
    "            try:\n",
    "                example_wer = wer.compute(references=[example[transcription_key_in_dictionary]], predictions=[predicted_text])\n",
    "                example_cer = cer.compute(references=[example[transcription_key_in_dictionary]], predictions=[predicted_text])\n",
    "                total_wer += example_wer\n",
    "                total_cer += example_cer\n",
    "                count += 1\n",
    "            except:\n",
    "                print(\"error\")\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "        if count > 0:\n",
    "            average_wer = total_wer / count\n",
    "            average_cer = total_cer / count\n",
    "        else:\n",
    "            average_wer = float('inf')  # Indicates an issue if no audio was processed\n",
    "            average_cer = float('inf')  # Indicates an issue if no audio was processed\n",
    "        return average_wer, average_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "class ASRModel:\n",
    "    def __init__(self, model_name=None, model=None, processor=None, feature_extractor=None, tokenizer=None, lm_model=False, device = None):\n",
    "        self.model_name=model_name\n",
    "        self.feature_extractor=feature_extractor\n",
    "        self.processor=processor\n",
    "        self.tokenizer=tokenizer\n",
    "        self.lm_model=lm_model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if feature_extractor and tokenizer:\n",
    "            self.feature_extractor=feature_extractor\n",
    "            self.tokenizer=tokenizer\n",
    "            self.processor=AutoProcessor(feature_extractor=feature_extractor, processor=processor)\n",
    "\n",
    "        elif processor:\n",
    "            self.processor=processor\n",
    "        else:\n",
    "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        print('Getting Model...')\n",
    "        if lm_model:\n",
    "            self.model= AutoModelForCTC.from_pretrained(model_name)\n",
    "        elif model:\n",
    "           self.model=model \n",
    "        else:\n",
    "            self.model = AutoModelForCTC.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "            \n",
    "    def get_prediction(self, inputs, sampling_rate=16000, return_tensors=\"pt\"):\n",
    "        with torch.no_grad():\n",
    "            self.logits = self.model(inputs).logits\n",
    "        if self.lm_model:\n",
    "            return self.lm_model.batch_decode(self.logits.cpu().numpy()).text\n",
    "        else:\n",
    "            predicted_ids = torch.argmax(self.logits, dim=-1)\n",
    "            return self.processor.batch_decode(predicted_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wav2Vec2-LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/models/KenLM-Wav2Vec2-imvladikon-300m-ivritai/language_model/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at imvladikon/wav2vec2-xls-r-300m-hebrew and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# processor = AutoProcessor.from_pretrained(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")\n",
    "# model = AutoModelForCTC.from_pretrained(\"imvladikon/wav2vec2-xls-r-300m-hebrew\")\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "lm = Wav2Vec2ProcessorWithLM.from_pretrained(\"/teamspace/studios/this_studio/models/KenLM-Wav2Vec2-imvladikon-300m-ivritai\")\n",
    "wav2vec_he_lm = ASRModel(\"imvladikon/wav2vec2-xls-r-300m-hebrew\", lm_model=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|| 500/500 [17:07<00:00,  2.05s/ samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 0.28\n",
      "Average CER: 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_wer, average_cer = evaluate_model(dataset, wav2vec_he_lm)\n",
    "print(f\"Average WER: {average_wer:.2f}\")\n",
    "print(f\"Average CER: {average_cer:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wav2Vec2Bert-LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /teamspace/studios/this_studio/models/wav2vec2bert-ivritai/language_model/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model...\n"
     ]
    }
   ],
   "source": [
    "# model_path='models/facebook/w2v-bert-2.0-finetuned'\n",
    "# processor = Wav2Vec2BertProcessor.from_pretrained(model_path, \n",
    "#                                             unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "# model = Wav2Vec2BertForCTC.from_pretrained(\n",
    "#     model_path,\n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     vocab_size=len(processor.tokenizer),\n",
    "# )\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "processor = Wav2Vec2BertProcessor.from_pretrained('models/facebook/w2v-bert-2.0-finetuned', \n",
    "                                            unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    ")\n",
    "\n",
    "bertLM = Wav2Vec2ProcessorWithLM.from_pretrained(\"/teamspace/studios/this_studio/models/wav2vec2bert-ivritai\")\n",
    "wav2vec2BertLm = ASRModel(model_name=\"models/facebook/w2v-bert-2.0-finetuned\", processor=processor, lm_model=bertLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|| 500/500 [19:03<00:00,  2.29s/ samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 0.23\n",
      "Average CER: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "average_wer, average_cer = evaluate_model(dataset, wav2vec2BertLm)\n",
    "print(f\"Average WER: {average_wer:.2f}\")\n",
    "print(f\"Average CER: {average_cer:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-AI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "model_path = 'openai/whisper-large-v2'\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "if has_cuda:\n",
    "    model.to('cuda:0')\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|| 500/500 [24:57<00:00,  3.00s/ samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 0.28\n",
      "Average CER: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_wer, average_cer = evaluate_model(dataset, model, processor, whisper=True)\n",
    "print(f\"Average WER: {average_wer:.2f}\")\n",
    "print(f\"Average CER: {average_cer:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ivrit-ai whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea8bc457305433eacc40f230dfb1090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "model_path = 'ivrit-ai/whisper-large-v2-tuned'\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "model=model.to(device)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|| 500/500 [25:30<00:00,  3.06s/ samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 0.27\n",
      "Average CER: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_wer, average_cer = evaluate_model(dataset, model, processor, whisper=True)\n",
    "print(f\"Average WER: {average_wer:.2f}\")\n",
    "print(f\"Average CER: {average_cer:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': '281c3cf84bbed9fbdc383d3b3c89633b2a96b234403bb99a087fbaaacd8623e344c884839cbff5a68e37f88352b3e8139a3cbd3a43ae2e2d7f84af420a08dd2a',\n",
       " 'path': '/home/zeus/.cache/huggingface/datasets/downloads/extracted/cc1985eee14bdfd38a365480ef760ea8bf84f8b7d1c980d7b50a502369bdb0e5/he_train_0/common_voice_he_39666459.mp3',\n",
       " 'audio': {'path': '/home/zeus/.cache/huggingface/datasets/downloads/extracted/cc1985eee14bdfd38a365480ef760ea8bf84f8b7d1c980d7b50a502369bdb0e5/he_train_0/common_voice_he_39666459.mp3',\n",
       "  'array': array([ 6.77626358e-21, -7.45388994e-20, -1.01643954e-19, ...,\n",
       "          3.68106521e-05,  2.69596840e-05,  1.26023469e-05]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': ' 转 转 转注转 住转 注专',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 0,\n",
       " 'age': 'twenties',\n",
       " 'gender': 'male_masculine',\n",
       " 'accent': 'Natibe',\n",
       " 'locale': 'he',\n",
       " 'segment': '',\n",
       " 'variant': ''}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=dataset[3]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write(\"comparison_test.wav\", example['audio']['array'], 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_array, sampling_rate = example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"]\n",
    "speech_array = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=\"openai/whisper-large-v2\",\n",
    "  # chunk_length_s=30,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More tests to do:\n",
    "# 1) Test against Whisper ivrit-ai model\n",
    "# 2) Test against Models without LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transcription = pipe([speech_array], return_timestamps=True, generate_kwargs={\"task\": \"transcribe\", \"language\": \"<|he|>\"}, batch_size=1)\n",
    "example_wer = wer.compute(references=[example[transcription_key_in_dictionary]], predictions=[transcription[0]['text']])\n",
    "example_cer = cer.compute(references=[example[transcription_key_in_dictionary]], predictions=[transcription[0]['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  转 转 转注转 住转 专.\n",
      " 转 转 转注转 住转 注专\n"
     ]
    }
   ],
   "source": [
    "print(transcription[0]['text'])\n",
    "print(example[transcription_key_in_dictionary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:92\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_values'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspeech_array\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16_000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_values\u001b[49m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:94\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = processor([speech_array], sampling_rate=16_000).input_values\n",
    "features = torch.tensor(inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(features).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predictions = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   爪注  注抓 抓 专转   砖驻注 注 驻住拽 转 砖注 砖']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "专  住专 转 转 专 住 专注砖 砖  驻住注转 住转\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_hebrew_vowels(text):\n",
    "    return re.sub(\"[\\u05B0-\\u05C3\\u05C7\\u05C4]\", \"\", text)\n",
    "\n",
    "text_without_vowels = remove_hebrew_vowels(example['sentence'])\n",
    "print(text_without_vowels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12055/3798879709.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer = load_metric(\"wer\")\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/wer/wer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/cer/cer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988c0471f5dc4a76b6cd7cabf67b1074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wer = load_metric(\"wer\")\n",
    "cer = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1794871794871795"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cer.compute(references=[example[transcription_key_in_dictionary]], predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  住注 注 注爪 抓 专   砖驻注 注 驻住拽 转 住注 砖'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[transcription_key_in_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
