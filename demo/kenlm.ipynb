{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_metric, load_dataset\n",
    "from transformers import Wav2Vec2BertProcessor, Wav2Vec2BertForCTC, BatchFeature\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "import utils\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, AutoProcessor\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an n-gram with KenLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While large language models based on the Transformer architecture have become the standard in NLP, it is still very common to use an n-gram LM to boost speech recognition systems - as shown in Section 1.\n",
    "\n",
    "Looking again at Table 9 of Appendix C of the official Wav2Vec2 paper, it can be noticed that using a Transformer-based LM for decoding clearly yields better results than using an n-gram model, but the difference between n-gram and Transformer-based LM is much less significant than the difference between n-gram and no LM.\n",
    "\n",
    "E.g., for the large Wav2Vec2 checkpoint that was fine-tuned on 10min only, an n-gram reduces the word error rate (WER) compared to no LM by ca. 80% while a Transformer-based LM only reduces the WER by another 23% compared to the n-gram. This relative WER reduction becomes less, the more data the acoustic model has been trained on. E.g., for the large checkpoint a Transformer-based LM reduces the WER by merely 8% compared to an n-gram LM whereas the n-gram still yields a 21% WER reduction compared to no language model.\n",
    "\n",
    "The reason why an n-gram is preferred over a Transformer-based LM is that n-grams come at a significantly smaller computational cost. For an n-gram, retrieving the probability of a word given previous words is almost only as computationally expensive as querying a look-up table or tree-like data storage - i.e. it's very fast compared to modern Transformer-based language models that would require a full forward pass to retrieve the next word probabilities.\n",
    "\n",
    "For more information on how n-grams function and why they are (still) so useful for speech recognition, the reader is advised to take a look at this excellent summary from Stanford.\n",
    "\n",
    "Great, let's see step-by-step how to build an n-gram. We will use the popular KenLM library to do so. Let's start by installing the Ubuntu library prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "file = 'ivritai.txt'\n",
    "finetuned_model_path = \"imvladikon/wav2vec2-xls-r-300m-hebrew\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_wav2vec2=load_dataset(\"ivrit-ai/whisper-training\")\n",
    "dataset = dataset_for_wav2vec2['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing unecessary columns\n",
      "Removing Special Characters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33b5b6345ff44d7a700e34574c79263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11049 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#subsample 10 percent of data\n",
    "data = dataset.select(np.random.permutation(dataset.shape[0])[:int(len(dataset)*0.2)])\n",
    "data = utils.standardize_dataset(data)\n",
    "data = utils.drop_english_samples(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>גם בשר ודם יכול תמיד לשמוע ולראות אותנו ואולי אין שום דבר שהוא פרטי ו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>זאת אומרת, אם אנחנו מסתכלים על הטלפון האדום שהיה בין מוסקבה לוושינגטון,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>הרי מדובר פה בבזבוז זמן מוחלט של שני הצדדים.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>הוא אומר במפורש</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>יצא לי לדבר עם יעל געל שהיא בסאב והיא עושה את זה כבר הרבה מאוד שנים והרבה לפניי. אז אם כבר מדברים על חלוצות בתחום בישראל אז...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ועכשיו יש לפרק הזה עשרת אלפים הורדות</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>באיזשהו מקום, הציבור ה...ישראלי,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>במדע וביזמות, שווה מאוד. כן, המלצתי עליו פה פעם אחת, אז אני אוסיף את המלצה הזו ואת טרו בלאד, מה עוד?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>תראה יש לי הרבה מה להגיד על גיימסטופ קודם כל אחד הדברים שבאמת.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>נראה שחקן בוגר, כאילו לא ילד, ילד כזה שעלה מהנוער. הוא יוביל את מכבי פתח תקווה בשנה הבאה. מקף תחתית.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_random_elements(data.remove_columns([\"audio\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_into_sentences(text):\n",
    "    # This regex handles '.', '!', '?', '...' as sentence boundaries\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)(?<!\\.\\.\\.) +')\n",
    "    sentences = sentence_endings.split(text)\n",
    "    return sentences\n",
    "def clean_text(text):\n",
    "    # Keep only Hebrew letters, punctuation marks (.,!?,;:), and spaces\n",
    "    return re.sub(r'[^א-ת \\n]', '', text)\n",
    "\n",
    "# Open a text file to write the sentences\n",
    "with open(file, \"w\") as file:\n",
    "    for example in data:  # Adjust if your dataset has a different split\n",
    "        transcriptions = example[\"transcription\"]\n",
    "        # Split the transcription into sentences\n",
    "        sentences = split_into_sentences(transcriptions)\n",
    "        for sentence in sentences:\n",
    "            sentence = clean_text(sentence)\n",
    "            file.write(sentence.strip() + \"\\n\")\n",
    "            \n",
    "with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "שצריך ללמוד תורה ואחד לא אומר שצריך ללמוד תורה אחד\n",
      "\n",
      "\n",
      "\n",
      "אוקיי קודם כל הנושא של הביזנס מודל ו\n",
      "\n",
      "רעיון ששמעתי היה ש היה איזושהי קריצה לזה ש\n",
      "\n",
      "אבל יש להם חיסרון מרכזי וזו כמות עצומה של חישובים וזיכרון שנדרשים לביצוע הרשת\n",
      "\n",
      "זה היה פשוט איוולת הנושא הזה כן\n",
      "\n",
      "אתה מספר שם את הסיפור שמגיע שם היזם של פולגת\n",
      "\n",
      "הקדיש פרק שלם בספר שלו על למה הוא לא שמרן\n",
      "\n",
      "כלומר לא רק שהוא לא ישתעמם מהתנך אלא הוא ירצה לרוץ ולקרוא אותו עד הסוף יהיה לו קשה לסגור\n",
      "\n",
      "הייתי יכול ללמד את הגוף להתמודד עם אין סוף מחלות או שזה לא עובד ככה\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the first 10 rows of no punctuation text\n",
    "with open('ivritai.txt', 'r') as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyctcdecode in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from pyctcdecode) (1.26.3)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from pyctcdecode) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from pyctcdecode) (6.98.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before downloading and unpacking the KenLM repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  480k  100  480k    0     0  79643      0  0:00:06  0:00:06 --:--:--  109k\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "if platform.system() == 'Darwin':  # Darwin stands for MacOS\n",
    "    !curl -L https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
    "elif platform.system() == 'Linux':\n",
    "    !wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
    "else:\n",
    "    print(\"Unsupported operating system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KenLM is written in C++, so we'll make use of `cmake` to build the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: kenlm/build: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
    "!ls kenlm/build/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyctcdecode in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from pyctcdecode) (1.26.3)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from pyctcdecode) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from pyctcdecode) (6.98.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
      "Requirement already satisfied: kenlm in /Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode\n",
    "# !pip install kenlm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, as we can see, the executable functions have successfully been built under `kenlm/build/bin/`.\n",
    "\n",
    "KenLM by default computes an *n-gram* with [Kneser-Ney smooting](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing). All text data used to create the *n-gram* is expected to be stored in a text file.\n",
    "We download our dataset and save it as a `.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just have to run KenLM's `lmplz` command to build our *n-gram*, called `\"5gram.arpa\"`. As it's relatively common in speech recognition, we build a *5-gram* by passing the `-o 5` parameter.\n",
    "For more information on the different *n-gram* LM that can be built\n",
    "with KenLM, one can take a look at the [official website of KenLM](https://kheafield.com/code/kenlm/).\n",
    "\n",
    "Executing the command below might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: kenlm/build/bin/lmplz\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 <\"{file}\" > \"5gram.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show current directory\n",
    "!ls kenlm/build/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shua/Desktop/FinalProject/demo'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have built a *5-gram* LM! Let's inspect the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -20 5gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small problem that 🤗 Transformers will not be happy about later on.\n",
    "The *5-gram* correctly includes a \"Unknown\" or `<unk>`, as well as a *begin-of-sentence*, `<s>` token, but no *end-of-sentence*, `</s>` token.\n",
    "This sadly has to be corrected currently after the build.\n",
    "\n",
    "We can simply add the *end-of-sentence* token by adding the line `0 </s>  -0.11831701` below the *begin-of-sentence* token and increasing the `ngram 1` count by 1. Because the file has roughly 100 million lines, this command will take *ca.* 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"5gram.arpa\", \"r\") as read_file, open(f\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the corrected *5-gram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -20 5gram_correct.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shua/miniconda3/envs/ml/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoProcessor\n",
    "from transformers import AutoProcessor, Wav2Vec2BertProcessor\n",
    "processor = AutoProcessor.from_pretrained(finetuned_model_path,\n",
    "                                            unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the vocabulary of its tokenizer as it represents the `\"labels\"` of `pyctcdecode`'s `BeamSearchDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"labels\"` and the previously built `5gram_correct.arpa` file is all that's needed to build the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /Users/shua/Desktop/FinalProject/demo/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot read model '5gram_correct.arpa' (End of file Byte: 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32mkenlm.pyx:139\u001b[0m, in \u001b[0;36mkenlm.Model.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: End of file Byte: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyctcdecode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_ctcdecoder\n\u001b[0;32m----> 3\u001b[0m decoder \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_ctcdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msorted_vocab_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkenlm_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5gram_correct.arpa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pyctcdecode/decoder.py:907\u001b[0m, in \u001b[0;36mbuild_ctcdecoder\u001b[0;34m(labels, kenlm_model_path, unigrams, alpha, beta, unk_score_offset, lm_score_boundary)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_ctcdecoder\u001b[39m(\n\u001b[1;32m    885\u001b[0m     labels: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    886\u001b[0m     kenlm_model_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m     lm_score_boundary: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_SCORE_LM_BOUNDARY,\n\u001b[1;32m    892\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeamSearchDecoderCTC:\n\u001b[1;32m    893\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a BeamSearchDecoderCTC instance with main functionality.\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;124;03m        instance of BeamSearchDecoderCTC\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m     kenlm_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m kenlm_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mkenlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkenlm_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kenlm_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kenlm_model_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.arpa\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    909\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing arpa instead of binary LM file, decoder instantiation might be slow.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mkenlm.pyx:142\u001b[0m, in \u001b[0;36mkenlm.Model.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot read model '5gram_correct.arpa' (End of file Byte: 0)"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"5gram_correct.arpa\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely ignore the warning and all that is left to do now is to wrap the just created `decoder`, together with the processor's `tokenizer` and `feature_extractor` into a `Wav2Vec2ProcessorWithLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Combine an *n-gram* with Wav2Vec2**\n",
    "\n",
    "In a final step, we want to wrap the *5-gram* into a `Wav2Vec2ProcessorWithLM` object to make the *5-gram* boosted decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(finetuned_model_path)\n",
    "normal_processor = AutoProcessor.from_pretrained(finetuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = chunks[46]\n",
    "inputs = normal_processor(sample, sampling_rate=16000, return_tensors=\"pt\")\n",
    "# sf.write(\"test.wav\", sample, 16000)\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits\n",
    "#prediction with kenlm\n",
    "kenlm = processor_with_lm.batch_decode(logits.numpy()).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular prediction\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "word2vec = processor.batch_decode(predicted_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
